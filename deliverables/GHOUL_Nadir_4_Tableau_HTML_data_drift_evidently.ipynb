{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Analyse de Data Drift avec Evidently",
   "id": "9ef37b51fe7c09cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from requests import get, Response\n",
    "from hashlib import sha256\n",
    "from tqdm.notebook import tqdm\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Added imports for the new transformer\n",
    "from scipy import stats\n",
    "from random import sample\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from utils.image_inverter import save\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_validate, GridSearchCV, \\\n",
    "    ParameterGrid\n",
    "from mlflow import set_tracking_uri, log_metrics, log_figure\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "_cache_folder = Path('~/.cache/gn_p7').expanduser()\n",
    "_cache_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_ds_url = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip'\n",
    "\n",
    "graph_folder: Path = Path(\"./graphs\")\n",
    "random_state: int = 42\n",
    "\n",
    "\n",
    "def save_figure(figure: plt.Figure, folder: str, figure_name: str) -> None:\n",
    "    folder = graph_folder / folder\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    save(figure, folder / f'{figure_name}.png', close=True)\n",
    "\n",
    "\n",
    "def download(url: str) -> Path:\n",
    "    url_id: str = sha256(url.encode('utf-8')).hexdigest()\n",
    "    local_path: Path = _cache_folder / url_id\n",
    "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not local_path.exists():\n",
    "        tmp_path: Path = _cache_folder / (url_id + '.tmp')\n",
    "        res: Response = get(url, stream=True)\n",
    "        with tmp_path.open('wb') as f, tqdm(\n",
    "                total=int(res.headers.get('content-length')),\n",
    "                desc=f'Downloading {url}',\n",
    "                unit_scale=True) as q:\n",
    "            for chunk in res.iter_content(chunk_size=8192):\n",
    "                q.update(len(chunk))\n",
    "                f.write(chunk)\n",
    "        tmp_path.replace(local_path)\n",
    "    return local_path\n",
    "\n",
    "\n",
    "def download_zip_archive(url: str) -> Path:\n",
    "    \"\"\"Download a zip archive, extract it then return the folder containing its content\"\"\"\n",
    "    archive_path: Path = download(url)\n",
    "    archive_folder: Path = Path(archive_path.as_posix() + '.dir')\n",
    "\n",
    "    if not archive_folder.exists():\n",
    "        print(f'Extracting archive {url}...', flush=True)\n",
    "        archive_temp: Path = Path(archive_path.as_posix() + '.tmp')\n",
    "        archive_temp.mkdir(parents=True, exist_ok=True)\n",
    "        archive: ZipFile = ZipFile(archive_path)\n",
    "        archive.extractall(path=archive_temp)\n",
    "        archive_temp.replace(archive_folder)\n",
    "        print(f'Extracting archive {url}...done', flush=True)\n",
    "\n",
    "    return archive_folder\n",
    "\n",
    "\n",
    "datasets: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "\n",
    "def get_dataset(name: str) -> pd.DataFrame:\n",
    "    folder = download_zip_archive(_ds_url)\n",
    "    if not name.endswith('.csv'):\n",
    "        name = f'{name}.csv'\n",
    "    try:\n",
    "        return datasets[name]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            _df = pd.read_csv(folder / name)\n",
    "        except FileNotFoundError:\n",
    "            display(Markdown(f'# ERROR: Dataset {name!r} not found, available datasets are:\\n' + '\\n'.join(\n",
    "                f'- {p.name}' for p in sorted(folder.iterdir(), key=(lambda x: x.name.lower())))))\n",
    "            raise KeyError(name) from None\n",
    "        else:\n",
    "            datasets[name] = _df\n",
    "            return _df.copy()\n"
   ],
   "id": "fead533dd4f17094",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from evidently import Report\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "from evidently.presets import DataDriftPreset\n",
    "\n",
    "# Chemin vers le zip téléchargé (adapte si nécessaire)\n",
    "archive_folder = Path('..')\n",
    "\n",
    "# Chargement des datasets\n",
    "app_train_df = get_dataset( 'application_train.csv')\n",
    "app_test_df = get_dataset('application_test.csv')\n",
    "\n",
    "# Hypothèse : app_train est la référence (données d'entraînement)\n",
    "# et app_test simule les nouvelles données en production\n",
    "reference_data = app_train_df.sample(n=20000, random_state=42)\n",
    "current_data = app_test_df.sample(n=20000, random_state=42)\n",
    "\n",
    "print(\"Données de référence chargées:\", reference_data.shape)\n",
    "print(\"Données courantes chargées:\", current_data.shape)\n",
    "\n",
    "(run := Report(metrics=[DataDriftPreset(), ]).run(reference_data=reference_data, current_data=current_data)).save_html(\n",
    "    'GHOUL_Nadir_4_Tableau_HTML_data_drift_evidently.html')\n",
    "\n",
    "run"
   ],
   "id": "24fa234aaf786bf9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
