{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from os import environ\n",
    "\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from mlflow import set_tracking_uri\n",
    "\n",
    "set_tracking_uri('http://127.65.12.247:50222')\n",
    "\n",
    "environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ],
   "id": "10884eb144621d83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Partie 0 - Constantes, imports et outils",
   "id": "ea725454e381bf1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from requests import get, Response\n",
    "from hashlib import sha256\n",
    "from tqdm.notebook import tqdm\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "\n",
    "from deliverables.utils.image_inverter import save\n",
    "\n",
    "_cache_folder = Path('~/.cache/gn_p7').expanduser()\n",
    "_cache_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_ds_url = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip'\n",
    "\n",
    "graph_folder: Path = Path(\"./graphs\")\n",
    "\n",
    "\n",
    "def save_figure(figure: plt.Figure, folder: str, figure_name: str) -> None:\n",
    "    folder = graph_folder / folder\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    save(figure, folder / f'{figure_name}.png', close=True)\n",
    "\n",
    "\n",
    "def download(url: str) -> Path:\n",
    "    url_id: str = sha256(url.encode('utf-8')).hexdigest()\n",
    "    local_path: Path = _cache_folder / url_id\n",
    "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not local_path.exists():\n",
    "        tmp_path: Path = _cache_folder / (url_id + '.tmp')\n",
    "        res: Response = get(url, stream=True)\n",
    "        with tmp_path.open('wb') as f, tqdm(\n",
    "                total=int(res.headers.get('content-length')),\n",
    "                desc=f'Downloading {url}',\n",
    "                unit_scale=True) as q:\n",
    "            for chunk in res.iter_content(chunk_size=8192):\n",
    "                q.update(len(chunk))\n",
    "                f.write(chunk)\n",
    "        tmp_path.replace(local_path)\n",
    "    return local_path\n",
    "\n",
    "\n",
    "def download_zip_archive(url: str) -> Path:\n",
    "    \"\"\"Download a zip archive, extract it then return the folder containing its content\"\"\"\n",
    "    archive_path: Path = download(url)\n",
    "    archive_folder: Path = Path(archive_path.as_posix() + '.dir')\n",
    "\n",
    "    if not archive_folder.exists():\n",
    "        print(f'Extracting archive {url}...', flush=True)\n",
    "        archive_temp: Path = Path(archive_path.as_posix() + '.tmp')\n",
    "        archive_temp.mkdir(parents=True, exist_ok=True)\n",
    "        archive: ZipFile = ZipFile(archive_path)\n",
    "        archive.extractall(path=archive_temp)\n",
    "        archive_temp.replace(archive_folder)\n",
    "        print(f'Extracting archive {url}...done', flush=True)\n",
    "\n",
    "    return archive_folder\n",
    "\n",
    "\n",
    "datasets: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "\n",
    "def get_dataset(name: str) -> pd.DataFrame:\n",
    "    folder = download_zip_archive(_ds_url)\n",
    "    if not name.endswith('.csv'):\n",
    "        name = f'{name}.csv'\n",
    "    try:\n",
    "        return datasets[name]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            _df = pd.read_csv(folder / name)\n",
    "        except FileNotFoundError:\n",
    "            display(Markdown(f'# ERROR: Dataset {name!r} not found, available datasets are:\\n' + '\\n'.join(\n",
    "                f'- {p.name}' for p in sorted(folder.iterdir(), key=(lambda x: x.name.lower())))))\n",
    "            raise KeyError(name) from None\n",
    "        else:\n",
    "            datasets[name] = _df\n",
    "            return _df.copy()\n"
   ],
   "id": "7909109ee36a25b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Partie 1 - EDA\n",
    "\n",
    "## Partie 1.1 - Chargement des données"
   ],
   "id": "749c0899a2112e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train, test = map(get_dataset, ('application_train', 'application_test'))",
   "id": "3b4a403e48353787",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train",
   "id": "ee5bfdc208e37108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test",
   "id": "94b8bc77b3c94295",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 1.2 - Analyse de la cible\n",
    "\n",
    "La cible est présente dans le dataset d'entrainement mais pas dans le dataset de test, pour éviter les fuites de données."
   ],
   "id": "af823897eb9c8eef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_figure(train.TARGET.value_counts().plot.pie(\n",
    "    title='Répartition des cibles (0=paiement complet, 1=retards de paiement)'\n",
    ").figure, '1_model', '0_target')"
   ],
   "id": "9b8122596bc8a39a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T14:22:38.277439Z",
     "start_time": "2025-08-18T14:22:38.275380Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Partie 1.3 - Analyse des features (hors cible)",
   "id": "457386fdd76c6259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to calculate missing values by column# Funct\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns={0: 'Missing Values', 1: '% of Total Values'})\n",
    "\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "                                                              \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "def missing_stats():\n",
    "    for fold in ('train', 'test'):\n",
    "        msno.matrix(get_dataset('application_' + fold + '.csv'), fontsize=12)\n",
    "        plt.title(f'Missing Values Count ({fold.title()}ing fold)', fontsize=16)\n",
    "        save_figure(plt.gcf(), '1_model', '1_missing_' + fold.title())\n",
    "\n",
    "    for fold in ('train', 'test'):\n",
    "        df = get_dataset('application_' + fold + '.csv')\n",
    "        msno.matrix(df[list(sorted(df.columns, key=(lambda col: int(df[col].notna().sum()))))], fontsize=12)\n",
    "        plt.title(f'Missing Values Count ({fold.title()}ing fold)', fontsize=16)\n",
    "        save_figure(plt.gcf(), '1_model', '2_sorted_missing_' + fold.title())\n",
    "\n",
    "    for fold in ('train', 'test'):\n",
    "        missing_test_values = missing_values_table(get_dataset('application_' + fold + '.csv'))\n",
    "\n",
    "        # TODO: Set the plot style for dark mode when exporting to png\n",
    "        plt.figure(figsize=(16, 12))  # There are a lot of columns\n",
    "        sns.barplot(x=missing_test_values['% of Total Values'], y=missing_test_values.index)\n",
    "        plt.title(f'Percentage of Missing Values by Feature ({fold.title()}ing fold)', fontsize=16)\n",
    "        plt.xlabel('% of Total Values', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "        # Add percentage text on the bars\n",
    "        for index, value in enumerate(missing_test_values['% of Total Values']):\n",
    "            plt.text(value, index, f' {value}%', va='center')\n",
    "\n",
    "        plt.xlim(0, 110)  # Set x-limit to give space for text\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), '1_model', '3_graph_missing' + fold.title())\n",
    "\n",
    "\n",
    "missing_stats()"
   ],
   "id": "8950fb1886cbc61c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nous pouvons voir qu'à peu près la moitié des colonnes manquent au moins une valeur, et que le reste est défini à environ 45-75%\n",
    "Si nous nous intéressons"
   ],
   "id": "ba9239ddc3e5446d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert not len(train.columns[\n",
    "                   (train.dtypes != 'int64') &\n",
    "                   (train.dtypes != 'float64') &\n",
    "                   (train.dtypes != 'object')]), 'Plus de types de colonnes sont présentes'\n",
    "display(Markdown('Il existe trois types de données en entrée, int64 et float64, numériques, et object, catégorielles'))\n",
    "display(\n",
    "    Markdown('Il arrive parfois que des données numériques soient accidentellement catégorisées en \"object\" si elles'\n",
    "             ' contiennent des valeurs non numérique, ce n\\'est pas le cas ici'))"
   ],
   "id": "dfc75f0a45c338b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 1.3 - Définition d'un pipeline de prétraitement\n",
    "\n",
    "L'avantage que le pipeline de prétraitement a est la robustesse contrer le data leakage, en effet les modèles d'apprentissages et de traitement seront entraîné sur les mêmes données, ce qui sera obligatoire pour pouvoir utiliser des techniques de K Fold en s'assurant que les folds soient indépendants les uns des autres.\n",
    "\n",
    "Note: Nous allons ignorer la transformation BoxCox en premier lieu\n",
    "\n",
    "Note relatives au déséquilibre de la cible:<br>\n",
    "Il existe un déséquilibre flagrant entre 1 et 0, avec 1 n'étant présent que pour 8% des données d'entraînement.<br>\n",
    "Pour y remédier, on a a notre disposition:\n",
    "- oversampling, par exemple SMOT: Synthetic Minority OverSampling Technique\n",
    "- undersampling\n",
    "- cost sensitive learning: Ajuster le scoring pour pénaliser davantage les faux négatifs\n",
    "\n",
    "\n",
    "\n",
    "TODO: Revoir s'il est nécessaire de tester les trois, ou si cost sensitive learning est suffisant\n"
   ],
   "id": "605048aa338f9d65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import wraps\n",
    "from typing import Callable, Any\n",
    "\n",
    "\n",
    "class OccupationPCA(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    ORGANIZATION_TYPE et OCCUPATION_TYPE donnent 78 colonnes après OneHot, ce qui complexifie le dataset.\n",
    "    En théorie cela nous donne 37 colonnes post pca\n",
    "    Pour minimiser le nombre de features, nous allons procéder de la manière suivante:\n",
    "    1. Application d'un One-Hot Encoding sur les colonnes 'ORGANIZATION_TYPE' et 'OCCUPATION_TYPE'.\n",
    "    2. Application d'une ACP pour conserver 95% de la variance, en nommant composantes 'OT_0', 'OT_1', etc.\n",
    "    5. Suppression des colonnes d'origine et ajout des nouvelles composantes au DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=0.95):\n",
    "        self.n_components = n_components  # Will either be a variance or number of components\n",
    "        self.pca = PCA(n_components=self.n_components)  # This will be \"fitted\" with the estimator\n",
    "\n",
    "        # To make sure the dataset holds the same shape, even if some values are missing from the testing dataset\n",
    "        self.dummy_columns = []\n",
    "\n",
    "    # TODO: Dummy NA?\n",
    "    def fit(self, X, y=None):\n",
    "        x = pd.get_dummies(X[['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']], dummy_na=True)\n",
    "        self.dummy_columns = x.columns\n",
    "        self.pca.fit(x)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x = X.copy()\n",
    "        x_org = pd.get_dummies(x[['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']], dummy_na=True)\n",
    "\n",
    "        # Add, remove and move around columns to get the same shape as the training set\n",
    "        x_org = x_org.reindex(columns=self.dummy_columns, fill_value=0)\n",
    "\n",
    "        pca_result = self.pca.transform(x_org)  # Apply pre-trained PCA\n",
    "\n",
    "        # Turning the results into a dataframe to include to x\n",
    "        pca_df = pd.DataFrame(pca_result, columns=[f\"OT_{i}\" for i in range(pca_result.shape[1])], index=x.index)\n",
    "\n",
    "        # Replacing old values with new pca values\n",
    "        return pd.concat([x.drop(columns=['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']), pca_df], axis=1)\n",
    "\n",
    "\n",
    "def copy_and_return_x[T, U](callback: Callable[[T, U], None]) -> Callable[[T, U], T]:\n",
    "    @wraps(callback)\n",
    "    def wrapped(x: T, y: U = None) -> T:\n",
    "        callback((x := x.copy()), y)\n",
    "        return x\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "@FunctionTransformer\n",
    "@copy_and_return_x\n",
    "def custom_preprocessor(X, y=None) -> None:\n",
    "    # On commence par réparer l'erreur que nous avons vu à l'étape d'Analyse Exploratoire\n",
    "    X['DAYS_EMPLOYED'] = X['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "    # On va ensuite supprimer la variable identifiant\n",
    "    X.pop('SK_ID_CURR')\n",
    "\n",
    "    # On va enfin traiter les valeurs catégorielles\n",
    "\n",
    "    # TODO: Vaut-il mieux utiliser trois colonnes?\n",
    "    X['CODE_GENDER'] = X['CODE_GENDER'].map({'F': 1, 'M': -1}).fillna(0)\n",
    "\n",
    "    for flag_name in X.columns[X.columns.str.startswith('FLAG_')]:\n",
    "        values = set(X[flag_name])\n",
    "        if {0, 1} - values:\n",
    "            continue\n",
    "        if {\"Y\", \"N\"} - values:\n",
    "            X[flag_name] = X[flag_name].map({'Y': 1, 'N': 0})\n",
    "        else:\n",
    "            raise RuntimeError(flag_name, values)\n",
    "\n",
    "\n",
    "@FunctionTransformer\n",
    "def dummy_preprocessor[XT](X: XT, y=None) -> XT:\n",
    "    return pd.get_dummies(X)\n",
    "\n",
    "\n",
    "# 2. Build the pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('custom_preprocessor', custom_preprocessor),\n",
    "    ('occupation_pca', OccupationPCA()),\n",
    "    ('dummy_preprocessor', dummy_preprocessor),\n",
    "], verbose=True)\n",
    "\n",
    "# We try to see if the pipeline works as expected\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "preprocessing_pipeline.fit(X_train, y_train)"
   ],
   "id": "19ad20a4eadb09c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 1.4: Définition de modèles de référence\n",
    "\n",
    "\n",
    "### Partie 1.4.1: DummyClassifier"
   ],
   "id": "a7a8de0b3a1aa682"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "dummy_classification = Pipeline(\n",
    "    deepcopy(preprocessing_pipeline.steps) + [\n",
    "        # 42 minutes for a random forest regressor\n",
    "        ('model', DummyClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "dummy_classification.fit(X_train, y_train)"
   ],
   "id": "3947a518d79f04f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Partie 1.4.2: DummyRegressor",
   "id": "be723d5c6aa6ac67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "dummy_regression = Pipeline(\n",
    "    deepcopy(preprocessing_pipeline.steps) + [\n",
    "        # 42 minutes for a random forest regressor\n",
    "        ('model', DummyRegressor())\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "dummy_regression.fit(X_train, y_train)\n"
   ],
   "id": "449fab4dc7c8aa1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Partie 1.4.3: LogisticRegression suivant un SimpleImputer",
   "id": "e2140fc068489ec0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Added imports for the new transformer\n",
    "from scipy import stats\n",
    "from random import sample\n",
    "\n",
    "# I don't want the BoxCox transformer to mess up data that it cannot fix.\n",
    "# TODO: Review how I estimate if it's worth the transformation\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "# TODO: Danger of this, if values are negatives and test values are smaller than train values, offset won't be enough\n",
    "class ConditionalBoxCox(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, p_value_threshold: float = 0.05):\n",
    "        self.p_value_threshold = p_value_threshold\n",
    "        self.columns_to_transform_ = []\n",
    "        self.learned = {}\n",
    "\n",
    "    def _as_numpy(self, X: pd.DataFrame | np.ndarray[tuple[int, ...], np.dtype[np.float64]]):\n",
    "        x: np.ndarray\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            for col in X.columns:\n",
    "                if X[col].isna().any():\n",
    "                    raise NotImplementedError(\n",
    "                        'I do not know yet how this would be handled,\\n\\t'\n",
    "                        'for now, you need this to follow an inputer to have no null values')\n",
    "            columns_to_drop = [i for i in X.columns if X[i].dtype == 'object']\n",
    "            columns_to_keep = [i for i in X.columns if X[i].dtype in ('float64', 'int64')]\n",
    "            if len(other_columns := set(X.columns) - set(columns_to_keep) - set(columns_to_drop)):\n",
    "                raise NotImplementedError(other_columns)\n",
    "            x = X.to_numpy()\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            x = X\n",
    "        else:\n",
    "            raise NotImplementedError(type(X))\n",
    "        return x\n",
    "\n",
    "    # noinspection PyTypeHints\n",
    "    def fit(self, X: pd.DataFrame | np.ndarray[tuple[int, ...], np.dtype[np.float64]], y=None):\n",
    "        x = self._as_numpy(X.copy())\n",
    "\n",
    "        for col, data in enumerate(x.T):\n",
    "            if len(set(data)) < 10:  # Not worth the change\n",
    "                continue\n",
    "            data = data + (shift := epsilon - (0 if (data > 0).all() else data.min()))\n",
    "            _, initial_p = stats.shapiro(sample(list(data), 100))\n",
    "            transformed_data, ld = stats.boxcox(data)\n",
    "            _, transformed_p = stats.shapiro(sample(list(transformed_data), 100))\n",
    "            if transformed_p > initial_p and transformed_p > self.p_value_threshold:\n",
    "                self.columns_to_transform_.append(col)\n",
    "                self.learned[col] = ld, shift\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x = self._as_numpy(X.copy())\n",
    "        for index in self.columns_to_transform_:\n",
    "            ld, shift = self.learned[index]\n",
    "            x[:, index] = stats.boxcox(\n",
    "                x[:, index] + shift, lmbda=ld)\n",
    "        return x\n",
    "\n",
    "\n",
    "logistic = Pipeline(\n",
    "    deepcopy(preprocessing_pipeline.steps) + [\n",
    "        # 17 seconds for this\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('coxbox', ConditionalBoxCox()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "logistic.fit(X_train, y_train)"
   ],
   "id": "1163d560ed3ffef5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Partie 1.4.4: LightGBM",
   "id": "95c6a4ac0a1b4b67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# Define the custom transformer\n",
    "class FeatureNameSanitizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        x = X.copy()\n",
    "        x.columns = [re.sub(r'[^A-Za-z0-9_]+', '', str(col)) for col in x.columns.tolist()]\n",
    "        return x\n",
    "\n",
    "\n",
    "lightgbm = Pipeline(\n",
    "    deepcopy(preprocessing_pipeline.steps) + [\n",
    "        ('sanitize_feature_names', FeatureNameSanitizer()),\n",
    "        # 42 minutes for a random forest regressor\n",
    "        ('model', LGBMClassifier(class_weight='balanced'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "lightgbm.fit(X_train, y_train)\n"
   ],
   "id": "7a1e2b0d37687f66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Partie 1.4.5: Comparison",
   "id": "b47886a6e656f120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "\n",
    "scores = dict(\n",
    "    (name, cross_val_score(pipeline, X_train, y_train, cv=RepeatedStratifiedKFold(\n",
    "        n_splits=5, n_repeats=10, random_state=42)))\n",
    "    for name, pipeline in (('DummyClassifier', dummy_classification), ('DummyRegressor', dummy_regression),\n",
    "                           ('LogisticRegression', logistic), ('LightGBMClassifier', lightgbm)))\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output(True)"
   ],
   "id": "a50485a1b29ea4ee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
