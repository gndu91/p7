{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from requests import get, Response\n",
    "from hashlib import sha256\n",
    "from tqdm.notebook import tqdm\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Added imports for the new transformer\n",
    "from scipy import stats\n",
    "from random import sample\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from deliverables.utils.image_inverter import save\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_validate\n",
    "from mlflow import set_tracking_uri\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ],
   "id": "10884eb144621d83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Partie 0 - Constantes, imports et outils",
   "id": "ea725454e381bf1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_cache_folder = Path('~/.cache/gn_p7').expanduser()\n",
    "_cache_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_ds_url = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip'\n",
    "\n",
    "graph_folder: Path = Path(\"./graphs\")\n",
    "\n",
    "\n",
    "def save_figure(figure: plt.Figure, folder: str, figure_name: str) -> None:\n",
    "    folder = graph_folder / folder\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    save(figure, folder / f'{figure_name}.png', close=True)\n",
    "\n",
    "\n",
    "def download(url: str) -> Path:\n",
    "    url_id: str = sha256(url.encode('utf-8')).hexdigest()\n",
    "    local_path: Path = _cache_folder / url_id\n",
    "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not local_path.exists():\n",
    "        tmp_path: Path = _cache_folder / (url_id + '.tmp')\n",
    "        res: Response = get(url, stream=True)\n",
    "        with tmp_path.open('wb') as f, tqdm(\n",
    "                total=int(res.headers.get('content-length')),\n",
    "                desc=f'Downloading {url}',\n",
    "                unit_scale=True) as q:\n",
    "            for chunk in res.iter_content(chunk_size=8192):\n",
    "                q.update(len(chunk))\n",
    "                f.write(chunk)\n",
    "        tmp_path.replace(local_path)\n",
    "    return local_path\n",
    "\n",
    "\n",
    "def download_zip_archive(url: str) -> Path:\n",
    "    \"\"\"Download a zip archive, extract it then return the folder containing its content\"\"\"\n",
    "    archive_path: Path = download(url)\n",
    "    archive_folder: Path = Path(archive_path.as_posix() + '.dir')\n",
    "\n",
    "    if not archive_folder.exists():\n",
    "        print(f'Extracting archive {url}...', flush=True)\n",
    "        archive_temp: Path = Path(archive_path.as_posix() + '.tmp')\n",
    "        archive_temp.mkdir(parents=True, exist_ok=True)\n",
    "        archive: ZipFile = ZipFile(archive_path)\n",
    "        archive.extractall(path=archive_temp)\n",
    "        archive_temp.replace(archive_folder)\n",
    "        print(f'Extracting archive {url}...done', flush=True)\n",
    "\n",
    "    return archive_folder\n",
    "\n",
    "\n",
    "datasets: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "\n",
    "def get_dataset(name: str) -> pd.DataFrame:\n",
    "    folder = download_zip_archive(_ds_url)\n",
    "    if not name.endswith('.csv'):\n",
    "        name = f'{name}.csv'\n",
    "    try:\n",
    "        return datasets[name]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            _df = pd.read_csv(folder / name)\n",
    "        except FileNotFoundError:\n",
    "            display(Markdown(f'# ERROR: Dataset {name!r} not found, available datasets are:\\n' + '\\n'.join(\n",
    "                f'- {p.name}' for p in sorted(folder.iterdir(), key=(lambda x: x.name.lower())))))\n",
    "            raise KeyError(name) from None\n",
    "        else:\n",
    "            datasets[name] = _df\n",
    "            return _df.copy()\n"
   ],
   "id": "7909109ee36a25b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Partie 1 - EDA\n",
    "\n",
    "## Partie 1.0 - Chargement des données"
   ],
   "id": "749c0899a2112e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train, test = map(get_dataset, ('application_train', 'application_test'))",
   "id": "3b4a403e48353787",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df = get_dataset('application_train')\n",
    "test_df = get_dataset('application_test')\n",
    "\n",
    "# Séparation des features et de la cible\n",
    "X = train_df.drop('TARGET', axis=1)\n",
    "y = train_df['TARGET']\n",
    "\n",
    "print(\"Données d'entraînement chargées :\", X.shape)\n",
    "print(\"Cible chargée :\", y.shape)"
   ],
   "id": "713d6b4c2579958d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train",
   "id": "ee5bfdc208e37108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test",
   "id": "94b8bc77b3c94295",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 1.1 - Analyse de la cible\n",
    "\n",
    "La cible est présente dans le dataset d'entrainement mais pas dans le dataset de test, pour éviter les fuites de données."
   ],
   "id": "af823897eb9c8eef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_figure(train.TARGET.value_counts().plot.pie(\n",
    "    title='Répartition des cibles (0=paiement complet, 1=retards de paiement)'\n",
    ").figure, '1_model', '0_target')"
   ],
   "id": "9b8122596bc8a39a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T14:22:38.277439Z",
     "start_time": "2025-08-18T14:22:38.275380Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Partie 1.3 - Analyse des features (hors cible)",
   "id": "457386fdd76c6259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to calculate missing values by column# Funct\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns={0: 'Missing Values', 1: '% of Total Values'})\n",
    "\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "                                                              \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "def missing_stats():\n",
    "    for fold in ('train', 'test'):\n",
    "        msno.matrix(get_dataset('application_' + fold + '.csv'), fontsize=12)\n",
    "        plt.title(f'Missing Values Count ({fold.title()}ing fold)', fontsize=16)\n",
    "        save_figure(plt.gcf(), '1_model', '1_missing_' + fold.title())\n",
    "\n",
    "    for fold in ('train', 'test'):\n",
    "        df = get_dataset('application_' + fold + '.csv')\n",
    "        msno.matrix(df[list(sorted(df.columns, key=(lambda col: int(df[col].notna().sum()))))], fontsize=12)\n",
    "        plt.title(f'Missing Values Count ({fold.title()}ing fold)', fontsize=16)\n",
    "        save_figure(plt.gcf(), '1_model', '2_sorted_missing_' + fold.title())\n",
    "\n",
    "    for fold in ('train', 'test'):\n",
    "        missing_test_values = missing_values_table(get_dataset('application_' + fold + '.csv'))\n",
    "\n",
    "        # TODO: Set the plot style for dark mode when exporting to png\n",
    "        plt.figure(figsize=(16, 12))  # There are a lot of columns\n",
    "        sns.barplot(x=missing_test_values['% of Total Values'], y=missing_test_values.index)\n",
    "        plt.title(f'Percentage of Missing Values by Feature ({fold.title()}ing fold)', fontsize=16)\n",
    "        plt.xlabel('% of Total Values', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "        # Add percentage text on the bars\n",
    "        for index, value in enumerate(missing_test_values['% of Total Values']):\n",
    "            plt.text(value, index, f' {value}%', va='center')\n",
    "\n",
    "        plt.xlim(0, 110)  # Set x-limit to give space for text\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), '1_model', '3_graph_missing' + fold.title())\n",
    "\n",
    "\n",
    "missing_stats()"
   ],
   "id": "8950fb1886cbc61c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nous pouvons voir qu'à peu près la moitié des colonnes manquent au moins une valeur, et que le reste est défini à environ 45-75%\n",
    "Si nous nous intéressons"
   ],
   "id": "ba9239ddc3e5446d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert not len(train.columns[\n",
    "                   (train.dtypes != 'int64') &\n",
    "                   (train.dtypes != 'float64') &\n",
    "                   (train.dtypes != 'object')]), 'Plus de types de colonnes sont présentes'\n",
    "display(Markdown('Il existe trois types de données en entrée, int64 et float64, numériques, et object, catégorielles'))\n",
    "display(\n",
    "    Markdown('Il arrive parfois que des données numériques soient accidentellement catégorisées en \"object\" si elles'\n",
    "             ' contiennent des valeurs non numérique, ce n\\'est pas le cas ici'))"
   ],
   "id": "dfc75f0a45c338b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 2 - Définition du Score Métier\n",
    "\n",
    "Nous somme face à deux déséquilibres:\n",
    "- Un déséquilibre des coûts (pb métier) : un faux négatif coûtant dix fois plus cher qu'un faux positif, il faut en tenir compte pour minimiser plus fortement les faux négatifs.\n",
    "    - Nous devons créer un `scorer` pour Scikit-Learn qui minimise le coût `coût = 10 * FN + 1 * FP`\n",
    "    - Il doit aussi trouver le seuil de décision optimal, car le seuil par défaut de 0.5 n'est probablement pas le meilleur d'un point de vue métier.\n",
    "- Un déséquilibre des classes (pb données) : 92% des cibles étant négatives, un risque serai accru d'avoir un modèle prédisant trops 0 (prédire 0 tout le temps nous donnera 92% de précision.\n",
    "    - Cela sera traité en utilisant comme paramètre `class_weight='balanced'`\n",
    "    - TODO: Don't forget to include it every time I introduce a new model"
   ],
   "id": "69f2fca80b91d4b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_optimal_threshold(y_true: np.ndarray[..., ...], y_pred_proba: np.ndarray[..., ...]):\n",
    "    \"\"\"Trouve le seuil qui minimise le coût métier.\"\"\"\n",
    "    thresholds = np.linspace(0.01, 0.99, 100)\n",
    "    costs = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_pred_proba[:, 1] >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "        costs.append(10 * fn + 1 * fp)\n",
    "\n",
    "    optimal_t_index = np.argmin(costs)\n",
    "    return thresholds[optimal_t_index], costs[optimal_t_index]\n",
    "\n",
    "\n",
    "def business_cost_scorer_func(y_true: np.ndarray[..., ...], y_pred_proba: np.ndarray[..., ...]):\n",
    "    \"\"\"Calcule le coût métier minimal en trouvant le meilleur seuil.\"\"\"\n",
    "    _, min_cost = find_optimal_threshold(y_true, y_pred_proba)\n",
    "    return min_cost\n",
    "\n",
    "\n",
    "def optimal_threshold_scorer_func(y_true: np.ndarray[..., ...], y_pred_proba: np.ndarray[..., ...]):\n",
    "    \"\"\"Retourne le seuil optimal qui minimise le coût métier.\"\"\"\n",
    "    optimal_t, _ = find_optimal_threshold(y_true, y_pred_proba)\n",
    "    return optimal_t\n",
    "\n",
    "\n",
    "# Création des scorers pour Scikit-Learn\n",
    "# On veut MINIMISER le coût, donc greater_is_better=False\n",
    "business_scorer = make_scorer(business_cost_scorer_func, greater_is_better=False)\n",
    "\n",
    "# Pour le seuil, c'est juste une information, donc pas de notion de 'meilleur'\n",
    "threshold_scorer = make_scorer(optimal_threshold_scorer_func)\n",
    "\n",
    "# On définit le dictionnaire de scoring qu'on utilisera dans la cross-validation\n",
    "scoring = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'business_cost': business_scorer,\n",
    "    'optimal_threshold': threshold_scorer\n",
    "}\n",
    "\n",
    "print(\"Scorers métier créés avec succès.\")"
   ],
   "id": "278b1da8e8fabd57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Partie 3 - Définition d'un pipeline de prétraitement\n",
    "\n",
    "L'avantage que le pipeline de prétraitement a est la robustesse contrer le data leakage, en effet les modèles d'apprentissages et de traitement seront entraîné sur les mêmes données, ce qui sera obligatoire pour pouvoir utiliser des techniques de K Fold en s'assurant que les folds soient indépendants les uns des autres.\n",
    "\n",
    "Note: Nous allons ignorer la transformation BoxCox en premier lieu\n",
    "\n",
    "Note relatives au déséquilibre de la cible:<br>\n",
    "Il existe un déséquilibre flagrant entre 1 et 0, avec 1 n'étant présent que pour 8% des données d'entraînement.<br>\n",
    "Pour y remédier, on a a notre disposition:\n",
    "- oversampling, par exemple SMOT: Synthetic Minority OverSampling Technique\n",
    "- undersampling\n",
    "- cost sensitive learning: Ajuster le scoring pour pénaliser davantage les faux négatifs\n",
    "\n",
    "\n",
    "\n",
    "TODO: Revoir s'il est nécessaire de tester les trois, ou si cost sensitive learning est suffisant\n"
   ],
   "id": "605048aa338f9d65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import wraps\n",
    "from typing import Callable, Any\n",
    "\n",
    "\n",
    "class OccupationPCA(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    ORGANIZATION_TYPE et OCCUPATION_TYPE donnent 78 colonnes après OneHot, ce qui complexifie le dataset.\n",
    "    En théorie cela nous donne 37 colonnes post pca\n",
    "    Pour minimiser le nombre de features, nous allons procéder de la manière suivante:\n",
    "    1. Application d'un One-Hot Encoding sur les colonnes 'ORGANIZATION_TYPE' et 'OCCUPATION_TYPE'.\n",
    "    2. Application d'une ACP pour conserver 95% de la variance, en nommant composantes 'OT_0', 'OT_1', etc.\n",
    "    5. Suppression des colonnes d'origine et ajout des nouvelles composantes au DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=0.95):\n",
    "        self.n_components = n_components  # Will either be a variance or number of components\n",
    "        self.pca = PCA(n_components=self.n_components)  # This will be \"fitted\" with the estimator\n",
    "\n",
    "        # To make sure the dataset holds the same shape, even if some values are missing from the testing dataset\n",
    "        self.dummy_columns = []\n",
    "\n",
    "    # TODO: Dummy NA?\n",
    "    def fit(self, X, y=None):\n",
    "        x = pd.get_dummies(X[['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']], dummy_na=True)\n",
    "        self.dummy_columns = x.columns\n",
    "        self.pca.fit(x)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x = X.copy()\n",
    "        x_org = pd.get_dummies(x[['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']], dummy_na=True)\n",
    "\n",
    "        # Add, remove and move around columns to get the same shape as the training set\n",
    "        x_org = x_org.reindex(columns=self.dummy_columns, fill_value=0)\n",
    "\n",
    "        pca_result = self.pca.transform(x_org)  # Apply pre-trained PCA\n",
    "\n",
    "        # Turning the results into a dataframe to include to x\n",
    "        pca_df = pd.DataFrame(pca_result, columns=[f\"OT_{i}\" for i in range(pca_result.shape[1])], index=x.index)\n",
    "\n",
    "        # Replacing old values with new pca values\n",
    "        return pd.concat([x.drop(columns=['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']), pca_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, handle_unknown='ignore', sparse_output=False):\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.sparse_output = sparse_output\n",
    "        self.ohe = OneHotEncoder(handle_unknown=self.handle_unknown,\n",
    "                                 sparse_output=self.sparse_output)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.categorical_columns_ = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        if self.categorical_columns_:\n",
    "            self.ohe.fit(X[self.categorical_columns_])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        if self.categorical_columns_:\n",
    "            encoded_data = self.ohe.transform(X[self.categorical_columns_])\n",
    "            new_cols = self.ohe.get_feature_names_out(self.categorical_columns_)\n",
    "            encoded_df = pd.DataFrame(encoded_data, columns=new_cols, index=X.index)\n",
    "            X_transformed = X_transformed.drop(columns=self.categorical_columns_)\n",
    "            X_transformed = pd.concat([X_transformed, encoded_df], axis=1)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "def copy_and_return_x[T, U](callback: Callable[[T, U], None]) -> Callable[[T, U], T]:\n",
    "    @wraps(callback)\n",
    "    def wrapped(x: T, y: U = None) -> T:\n",
    "        callback((x := x.copy()), y)\n",
    "        return x\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "@FunctionTransformer\n",
    "@copy_and_return_x\n",
    "def custom_preprocessor(X, y=None) -> None:\n",
    "    # On commence par réparer l'erreur que nous avons vu à l'étape d'Analyse Exploratoire\n",
    "    X['DAYS_EMPLOYED'] = X['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "    # On va ensuite supprimer la variable identifiant\n",
    "    X.pop('SK_ID_CURR')  # TODO: Check for this\n",
    "\n",
    "    # On va enfin traiter les valeurs catégorielles\n",
    "\n",
    "    # TODO: Vaut-il mieux utiliser trois colonnes?\n",
    "    X['CODE_GENDER'] = X['CODE_GENDER'].map({'F': 1, 'M': -1}).fillna(0)\n",
    "\n",
    "    for flag_name in X.columns[X.columns.str.startswith('FLAG_')]:\n",
    "        values = set(X[flag_name])\n",
    "        if {0, 1} - values:\n",
    "            continue\n",
    "        if {\"Y\", \"N\"} - values:\n",
    "            X[flag_name] = X[flag_name].map({'Y': 1, 'N': 0})\n",
    "        else:\n",
    "            raise RuntimeError(flag_name, values)\n",
    "\n",
    "\n",
    "@FunctionTransformer\n",
    "@copy_and_return_x\n",
    "def sanitize_feature_names(x, y=None):\n",
    "    x.columns = [re.sub(r'[^A-Za-z0-9_]+', '', str(col)) for col in x.columns.tolist()]\n",
    "\n",
    "\n",
    "# 2. Build the pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('custom_preprocessor', custom_preprocessor),\n",
    "    ('occupation_pca', OccupationPCA()),\n",
    "    ('first_name_sanitization', sanitize_feature_names),\n",
    "    ('dummy_preprocessor', CategoricalEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "    ('second_name_sanitization', sanitize_feature_names),\n",
    "], verbose=True)\n",
    "\n",
    "# We try to see if the pipeline works as expected\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "preprocessing_pipeline.fit_transform(X_train, y_train)"
   ],
   "id": "19ad20a4eadb09c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Partie 4: Outils",
   "id": "3e32cb43db886368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Danger of this, if values are negatives and test values are smaller than train values, offset won't be enough\n",
    "# I don't want the BoxCox transformer to mess up data that it cannot fix.\n",
    "# TODO: Review how I estimate if it's worth the transformation\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "class ConditionalBoxCox(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, p_value_threshold: float = 0.05):\n",
    "        self.p_value_threshold = p_value_threshold\n",
    "        self.columns_to_transform_ = []\n",
    "        self.learned = {}\n",
    "\n",
    "    def _as_numpy(self, X: pd.DataFrame | np.ndarray[tuple[int, ...], np.dtype[np.float64]]):\n",
    "        x: np.ndarray\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            for col in X.columns:\n",
    "                if X[col].isna().any():\n",
    "                    raise NotImplementedError(\n",
    "                        'I do not know yet how this would be handled,\\n\\t'\n",
    "                        'for now, you need this to follow an inputer to have no null values')\n",
    "            columns_to_drop = [i for i in X.columns if X[i].dtype == 'object']\n",
    "            columns_to_keep = [i for i in X.columns if X[i].dtype in ('float64', 'int64')]\n",
    "            if len(other_columns := set(X.columns) - set(columns_to_keep) - set(columns_to_drop)):\n",
    "                raise NotImplementedError(other_columns)\n",
    "            x = X.to_numpy()\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            x = X\n",
    "        else:\n",
    "            raise NotImplementedError(type(X))\n",
    "        return x\n",
    "\n",
    "    # noinspection PyTypeHints\n",
    "    def fit(self, X: pd.DataFrame | np.ndarray[tuple[int, ...], np.dtype[np.float64]], y=None):\n",
    "        x = self._as_numpy(X.copy())\n",
    "\n",
    "        for col, data in enumerate(x.T):\n",
    "            if len(set(data)) < 10:  # Not worth the change\n",
    "                continue\n",
    "            data = data + (shift := epsilon - (0 if (data > 0).all() else data.min()))\n",
    "            _, initial_p = stats.shapiro(sample(list(data), 100))\n",
    "            transformed_data, ld = stats.boxcox(data)\n",
    "            _, transformed_p = stats.shapiro(sample(list(transformed_data), 100))\n",
    "            if transformed_p > initial_p and transformed_p > self.p_value_threshold:\n",
    "                self.columns_to_transform_.append(col)\n",
    "                self.learned[col] = ld, shift\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x = self._as_numpy(X.copy())\n",
    "        for index in self.columns_to_transform_:\n",
    "            ld, shift = self.learned[index]\n",
    "            x[:, index] = stats.boxcox(\n",
    "                x[:, index] + shift, lmbda=ld)\n",
    "        return x\n",
    "\n",
    "\n",
    "def new_pipeline(*steps):\n",
    "    return Pipeline(deepcopy(preprocessing_pipeline.steps) + list(steps))"
   ],
   "id": "fbf1019f6ecf0d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 5: Pipelines de modélisation\n",
    "\n",
    "Nous allons maintenant définir nos modèles et lancer les expérimentations. Chaque modèle sera testé dans un `run` MLflow distinct.\n",
    "\n",
    "Pour chaque `run`, nous allons logger :\n",
    "1.  **Les paramètres** : Tous les hyperparamètres du pipeline.\n",
    "2.  **Les métriques** : Les scores (AUC, coût métier, seuil) calculés par validation croisée (moyenne et écart-type).\n",
    "3.  **Le modèle final** : Le pipeline complet, entraîné sur l'ensemble des données, prêt à être déployé.\n",
    "4.  **(Optionnel) Des artefacts** : Comme des graphiques (ex: feature importance)."
   ],
   "id": "a32dfb4270f46cbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from mlflow import set_experiment, start_run, log_params, log_metric, log_param, models, sklearn\n",
    "\n",
    "# TODO: `random_state`\n",
    "\n",
    "# Définition des modèles de prétraitement\n",
    "pipelines = [\n",
    "    ('DummyClassifier', new_pipeline(('model', DummyClassifier()))),\n",
    "    ('LogisticRegression', new_pipeline(\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('coxbox', ConditionalBoxCox()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(class_weight='balanced')))),\n",
    "    ('LightGBMClassifier', new_pipeline(('model', LGBMClassifier(class_weight='balanced'))))\n",
    "]\n",
    "\n",
    "# Définition de la stratégie de validation croisée\n",
    "cv_strategy = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "\n",
    "display(Markdown(f\"***{len(pipelines)}*** modèles sont prêts à être testés.\"))\n",
    "\n",
    "# MLFlow\n",
    "set_tracking_uri('http://127.65.12.247:50222')\n",
    "set_experiment(\"P7_Credit_Scoring_Models\")\n",
    "\n",
    "# Boucle d'expérimentation\n",
    "for model_name, pipeline in pipelines:\n",
    "    print(f\"--- Lancement du run pour le modèle : {model_name} ---\")\n",
    "\n",
    "    with start_run(run_name=model_name) as run:\n",
    "        # 1. Log des paramètres\n",
    "        # On log les paramètres pertinents de manière explicite pour plus de clarté\n",
    "        log_params(pipeline.get_params()['model'].get_params())\n",
    "        log_param(\"pipeline_steps\", [step[0] for step in pipeline.steps])\n",
    "\n",
    "        # 2. Entraînement et évaluation par validation croisée\n",
    "        cv_results = cross_validate(pipeline, X, y, cv=cv_strategy, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "        # 3. Log des métriques (moyenne et écart-type pour chaque score)\n",
    "        for score_name, values in cv_results.items():\n",
    "            if 'test_' in score_name:\n",
    "                metric_name = score_name.replace('test_', '')\n",
    "                log_metric(f\"{metric_name}_mean\", values.mean())\n",
    "                log_metric(f\"{metric_name}_std\", values.std())\n",
    "            if 'time' in score_name:\n",
    "                log_metric(f\"{score_name}_mean\", values.mean())\n",
    "                log_metric(f\"{score_name}_std\", values.std())\n",
    "\n",
    "        print(f\"Coût métier moyen : {-cv_results['test_business_cost'].mean():.2f}\")\n",
    "        print(f\"AUC moyen : {cv_results['test_roc_auc'].mean():.3f}\")\n",
    "\n",
    "        # 4. Entraînement du modèle final sur toutes les données\n",
    "        pipeline.fit(X, y)\n",
    "\n",
    "        # 5. Log du modèle avec sa signature\n",
    "        # La signature aide MLflow à comprendre le format d'entrée/sortie\n",
    "        sklearn.log_model(\n",
    "            sk_model=pipeline,\n",
    "            signature=models.infer_signature(X.head(), pipeline.predict_proba(X.head()))\n",
    "        )\n",
    "\n",
    "    print(f\"--- Run pour {model_name} terminé et loggé. ---\\n\")"
   ],
   "id": "7a1e2b0d37687f66",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
