{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from requests import get, Response\n",
    "from hashlib import sha256\n",
    "from tqdm.notebook import tqdm\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Added imports for the new transformer\n",
    "from scipy import stats\n",
    "from random import sample\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from deliverables.utils.image_inverter import save\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_validate, GridSearchCV, \\\n",
    "    ParameterGrid\n",
    "from mlflow import set_tracking_uri, log_metrics, log_figure\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ],
   "id": "cfb6a2fa6e36e0b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Partie 0 - Constantes, imports et outils",
   "id": "e438bb10601c63b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_cache_folder = Path('~/.cache/gn_p7').expanduser()\n",
    "_cache_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_ds_url = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip'\n",
    "\n",
    "graph_folder: Path = Path(\"./graphs\")\n",
    "random_state: int = 42\n",
    "\n",
    "\n",
    "def save_figure(figure: plt.Figure, folder: str, figure_name: str) -> None:\n",
    "    folder = graph_folder / folder\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    save(figure, folder / f'{figure_name}.png', close=True)\n",
    "\n",
    "\n",
    "def download(url: str) -> Path:\n",
    "    url_id: str = sha256(url.encode('utf-8')).hexdigest()\n",
    "    local_path: Path = _cache_folder / url_id\n",
    "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not local_path.exists():\n",
    "        tmp_path: Path = _cache_folder / (url_id + '.tmp')\n",
    "        res: Response = get(url, stream=True)\n",
    "        with tmp_path.open('wb') as f, tqdm(\n",
    "                total=int(res.headers.get('content-length')),\n",
    "                desc=f'Downloading {url}',\n",
    "                unit_scale=True) as q:\n",
    "            for chunk in res.iter_content(chunk_size=8192):\n",
    "                q.update(len(chunk))\n",
    "                f.write(chunk)\n",
    "        tmp_path.replace(local_path)\n",
    "    return local_path\n",
    "\n",
    "\n",
    "def download_zip_archive(url: str) -> Path:\n",
    "    \"\"\"Download a zip archive, extract it then return the folder containing its content\"\"\"\n",
    "    archive_path: Path = download(url)\n",
    "    archive_folder: Path = Path(archive_path.as_posix() + '.dir')\n",
    "\n",
    "    if not archive_folder.exists():\n",
    "        print(f'Extracting archive {url}...', flush=True)\n",
    "        archive_temp: Path = Path(archive_path.as_posix() + '.tmp')\n",
    "        archive_temp.mkdir(parents=True, exist_ok=True)\n",
    "        archive: ZipFile = ZipFile(archive_path)\n",
    "        archive.extractall(path=archive_temp)\n",
    "        archive_temp.replace(archive_folder)\n",
    "        print(f'Extracting archive {url}...done', flush=True)\n",
    "\n",
    "    return archive_folder\n",
    "\n",
    "\n",
    "datasets: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "\n",
    "def get_dataset(name: str) -> pd.DataFrame:\n",
    "    folder = download_zip_archive(_ds_url)\n",
    "    if not name.endswith('.csv'):\n",
    "        name = f'{name}.csv'\n",
    "    try:\n",
    "        return datasets[name]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            _df = pd.read_csv(folder / name)\n",
    "        except FileNotFoundError:\n",
    "            display(Markdown(f'# ERROR: Dataset {name!r} not found, available datasets are:\\n' + '\\n'.join(\n",
    "                f'- {p.name}' for p in sorted(folder.iterdir(), key=(lambda x: x.name.lower())))))\n",
    "            raise KeyError(name) from None\n",
    "        else:\n",
    "            datasets[name] = _df\n",
    "            return _df.copy()\n"
   ],
   "id": "52eebf7590d1f3bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Partie 1 - EDA\n",
    "\n",
    "## Partie 1.0 - Chargement des données"
   ],
   "id": "da553995ee1ac61f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = map(get_dataset, ('application_train', 'application_test'))\n",
    "train_ratio = len(train) / (len(train) + len(test))\n",
    "test_ratio = len(test) / (len(train) + len(test))\n",
    "train_df, test_df = train, test = train_test_split(\n",
    "    train,\n",
    "    test_size=test_ratio,\n",
    "    random_state=random_state,\n",
    "    stratify=train['TARGET']\n",
    ")"
   ],
   "id": "9230c95ae363f990",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Séparation des features et de la cible\n",
    "X = train_df.drop('TARGET', axis=1)\n",
    "y = train_df['TARGET']\n",
    "\n",
    "print(\"Données d'entraînement chargées :\", X.shape)\n",
    "print(\"Cible chargée :\", y.shape)"
   ],
   "id": "103c06f72bb9e2ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Séparation des features et de la cible\n",
    "X = test_df.drop('TARGET', axis=1)\n",
    "y = test_df['TARGET']\n",
    "\n",
    "print(\"Données de test chargées :\", X.shape)\n",
    "print(\"Cible chargée :\", y.shape)"
   ],
   "id": "8e6fb843faf25758",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train",
   "id": "161854566122bfae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test",
   "id": "2099de0fe2561b2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 1.1 - Analyse de la cible\n",
    "\n",
    "La cible est présente dans le dataset d'entrainement mais pas dans le dataset de test, pour éviter les fuites de données."
   ],
   "id": "1a216db8d58ab4fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_figure(train.TARGET.value_counts().plot.pie(\n",
    "    title='Répartition des cibles (0=paiement complet, 1=retards de paiement)'\n",
    ").figure, '1_model', '0_target')"
   ],
   "id": "5cd5272235804ee7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Partie 1.3 - Analyse des features (hors cible)",
   "id": "550bc328db329346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to calculate missing values by column# Funct\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns={0: 'Missing Values', 1: '% of Total Values'})\n",
    "\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "                                                              \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "def missing_stats():\n",
    "    for fold in ('train', 'test'):\n",
    "        msno.matrix(get_dataset('application_' + fold + '.csv'), fontsize=12)\n",
    "        plt.title(f'Missing Values Count ({fold.title()}ing fold)', fontsize=16)\n",
    "        save_figure(plt.gcf(), '1_model', '1_missing_' + fold.title())\n",
    "\n",
    "    for fold in ('train', 'test'):\n",
    "        df = get_dataset('application_' + fold + '.csv')\n",
    "        msno.matrix(df[list(sorted(df.columns, key=(lambda col: int(df[col].notna().sum()))))], fontsize=12)\n",
    "        plt.title(f'Missing Values Count ({fold.title()}ing fold)', fontsize=16)\n",
    "        save_figure(plt.gcf(), '1_model', '2_sorted_missing_' + fold.title())\n",
    "\n",
    "    for fold in ('train', 'test'):\n",
    "        missing_test_values = missing_values_table(get_dataset('application_' + fold + '.csv'))\n",
    "\n",
    "        # TODO: Set the plot style for dark mode when exporting to png\n",
    "        plt.figure(figsize=(16, 12))  # There are a lot of columns\n",
    "        sns.barplot(x=missing_test_values['% of Total Values'], y=missing_test_values.index)\n",
    "        plt.title(f'Percentage of Missing Values by Feature ({fold.title()}ing fold)', fontsize=16)\n",
    "        plt.xlabel('% of Total Values', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "        # Add percentage text on the bars\n",
    "        for index, value in enumerate(missing_test_values['% of Total Values']):\n",
    "            plt.text(value, index, f' {value}%', va='center')\n",
    "\n",
    "        plt.xlim(0, 110)  # Set x-limit to give space for text\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), '1_model', '3_graph_missing' + fold.title())\n",
    "\n",
    "\n",
    "missing_stats()"
   ],
   "id": "6a22f787f2127f61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nous pouvons voir qu'à peu près la moitié des colonnes manquent au moins une valeur, et que le reste est défini à environ 45-75%\n",
    "Si nous nous intéressons"
   ],
   "id": "e54113cada4b4976"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert not len(train.columns[\n",
    "                   (train.dtypes != 'int64') &\n",
    "                   (train.dtypes != 'float64') &\n",
    "                   (train.dtypes != 'object')]), 'Plus de types de colonnes sont présentes'\n",
    "display(Markdown('Il existe trois types de données en entrée, int64 et float64, numériques, et object, catégorielles'))\n",
    "display(\n",
    "    Markdown('Il arrive parfois que des données numériques soient accidentellement catégorisées en \"object\" si elles'\n",
    "             ' contiennent des valeurs non numérique, ce n\\'est pas le cas ici'))"
   ],
   "id": "dc8d94dc719297ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 2 - Définition du Score Métier\n",
    "\n",
    "Nous somme face à deux déséquilibres:\n",
    "- Un déséquilibre des coûts (pb métier) : un faux négatif coûtant dix fois plus cher qu'un faux positif, il faut en tenir compte pour minimiser plus fortement les faux négatifs.\n",
    "    - Nous devons créer un `scorer` pour Scikit-Learn qui minimise le coût `coût = 10 * FN + 1 * FP`\n",
    "    - Il doit aussi trouver le seuil de décision optimal, car le seuil par défaut de 0.5 n'est probablement pas le meilleur d'un point de vue métier.\n",
    "- Un déséquilibre des classes (pb données) : 92% des cibles étant négatives, un risque serai accru d'avoir un modèle prédisant trops 0 (prédire 0 tout le temps nous donnera 92% de précision.\n",
    "    - Cela sera traité en utilisant comme paramètre `class_weight='balanced'`\n",
    "    - TODO: Don't forget to include it every time I introduce a new model"
   ],
   "id": "92c44add145e5c8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_optimal_threshold(y_true: np.ndarray, y_pred_proba: np.ndarray):\n",
    "    \"\"\"Trouve le seuil qui minimise le coût métier.\"\"\"\n",
    "    # Si y_pred_proba est 2D, on prend la probabilité de la classe 1\n",
    "    if y_pred_proba.ndim == 2:\n",
    "        y_scores = y_pred_proba[:, 1]\n",
    "    # Sinon, on suppose que c'est déjà le score pour la classe positive\n",
    "    else:\n",
    "        y_scores = y_pred_proba\n",
    "\n",
    "    thresholds = np.linspace(0.01, 0.99, 100)\n",
    "    costs = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_scores >= t).astype(int)  # On utilise y_scores ici\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "        costs.append(10 * fn + 1 * fp)\n",
    "\n",
    "    optimal_t_index = np.argmin(costs)\n",
    "    return thresholds[optimal_t_index], costs[optimal_t_index]\n",
    "\n",
    "\n",
    "def business_cost_scorer_func(y_true: np.ndarray[..., ...], y_pred_proba: np.ndarray[..., ...]):\n",
    "    \"\"\"Calcule le coût métier minimal en trouvant le meilleur seuil.\"\"\"\n",
    "    _, min_cost = find_optimal_threshold(y_true, y_pred_proba)\n",
    "    return min_cost / len(y_true)\n",
    "\n",
    "\n",
    "def optimal_threshold_scorer_func(y_true: np.ndarray[..., ...], y_pred_proba: np.ndarray[..., ...]):\n",
    "    \"\"\"Retourne le seuil optimal qui minimise le coût métier.\"\"\"\n",
    "    optimal_t, _ = find_optimal_threshold(y_true, y_pred_proba)\n",
    "    return optimal_t\n",
    "\n",
    "\n",
    "# Création des scorers pour Scikit-Learn\n",
    "# On veut MINIMISER le coût, donc greater_is_better=False\n",
    "business_scorer = make_scorer(business_cost_scorer_func, greater_is_better=False, response_method='predict_proba')\n",
    "\n",
    "# Pour le seuil, c'est juste une information, donc pas de notion de 'meilleur'\n",
    "threshold_scorer = make_scorer(optimal_threshold_scorer_func, response_method='predict_proba')\n",
    "\n",
    "# On définit le dictionnaire de scoring qu'on utilisera dans la cross-validation\n",
    "scoring = {\n",
    "    'business_cost': business_scorer,\n",
    "    'business_threshold': threshold_scorer,\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1': 'f1',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall'\n",
    "}\n",
    "print(\"Scorers métier créés avec succès.\")"
   ],
   "id": "5d86bc44377ae63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Partie 3 - Définition d'un pipeline de prétraitement\n",
    "\n",
    "L'avantage que le pipeline de prétraitement a est la robustesse contrer le data leakage, en effet les modèles d'apprentissages et de traitement seront entraîné sur les mêmes données, ce qui sera obligatoire pour pouvoir utiliser des techniques de K Fold en s'assurant que les folds soient indépendants les uns des autres.\n",
    "\n",
    "Note: Nous allons ignorer la transformation BoxCox en premier lieu\n",
    "\n",
    "Note relatives au déséquilibre de la cible:<br>\n",
    "Il existe un déséquilibre flagrant entre 1 et 0, avec 1 n'étant présent que pour 8% des données d'entraînement.<br>\n",
    "Pour y remédier, on a a notre disposition:\n",
    "- oversampling, par exemple SMOT: Synthetic Minority OverSampling Technique\n",
    "- undersampling\n",
    "- cost sensitive learning: Ajuster le scoring pour pénaliser davantage les faux négatifs\n",
    "\n",
    "\n",
    "\n",
    "TODO: Revoir s'il est nécessaire de tester les trois, ou si cost sensitive learning est suffisant\n"
   ],
   "id": "79264bad67f6fe58"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "cc = get_dataset('credit_card_balance')\n",
    "cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "# General aggregations\n",
    "cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "# Count credit card lines\n",
    "cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "cc_agg\n"
   ],
   "id": "a8db636c9aeed66c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import wraps\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class OccupationPCA(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    ORGANIZATION_TYPE et OCCUPATION_TYPE donnent 78 colonnes après OneHot, ce qui complexifie le dataset.\n",
    "    En théorie cela nous donne 37 colonnes post pca\n",
    "    Pour minimiser le nombre de features, nous allons procéder de la manière suivante:\n",
    "    1. Application d'un One-Hot Encoding sur les colonnes 'ORGANIZATION_TYPE' et 'OCCUPATION_TYPE'.\n",
    "    2. Application d'une ACP pour conserver 95% de la variance, en nommant composantes 'OT_0', 'OT_1', etc.\n",
    "    5. Suppression des colonnes d'origine et ajout des nouvelles composantes au DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=0.95):\n",
    "        self.n_components = n_components  # Will either be a variance or number of components\n",
    "        self.pca = PCA(n_components=self.n_components)  # This will be \"fitted\" with the estimator\n",
    "\n",
    "        # To make sure the dataset holds the same shape, even if some values are missing from the testing dataset\n",
    "        self.dummy_columns = []\n",
    "\n",
    "    # TODO: Dummy NA?\n",
    "    def fit(self, X, y=None):\n",
    "        x = pd.get_dummies(X[['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']], dummy_na=True)\n",
    "        self.dummy_columns = x.columns\n",
    "        self.pca.fit(x)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x = X.copy()\n",
    "        x_org = pd.get_dummies(x[['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']], dummy_na=True)\n",
    "\n",
    "        # Add, remove and move around columns to get the same shape as the training set\n",
    "        x_org = x_org.reindex(columns=self.dummy_columns, fill_value=0)\n",
    "\n",
    "        pca_result = self.pca.transform(x_org)  # Apply pre-trained PCA\n",
    "\n",
    "        # Turning the results into a dataframe to include to x\n",
    "        pca_df = pd.DataFrame(pca_result, columns=[f\"OT_{i}\" for i in range(pca_result.shape[1])], index=x.index)\n",
    "\n",
    "        # Replacing old values with new pca values\n",
    "        return pd.concat([x.drop(columns=['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']), pca_df], axis=1)\n",
    "\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, handle_unknown='ignore', sparse_output=False):\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.sparse_output = sparse_output\n",
    "        self.ohe = OneHotEncoder(handle_unknown=self.handle_unknown,\n",
    "                                 sparse_output=self.sparse_output)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.categorical_columns_ = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        if self.categorical_columns_:\n",
    "            self.ohe.fit(X[self.categorical_columns_])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        if self.categorical_columns_:\n",
    "            encoded_data = self.ohe.transform(X[self.categorical_columns_])\n",
    "            new_cols = self.ohe.get_feature_names_out(self.categorical_columns_)\n",
    "            encoded_df = pd.DataFrame(encoded_data, columns=new_cols, index=X.index)\n",
    "            X_transformed = X_transformed.drop(columns=self.categorical_columns_)\n",
    "            X_transformed = pd.concat([X_transformed, encoded_df], axis=1)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "def copy_and_return_x[T, U](callback: Callable[[T, U], None]) -> Callable[[T, U], T]:\n",
    "    @wraps(callback)\n",
    "    def wrapped(x: T, y: U = None) -> T:\n",
    "        callback((x := x.copy()), y)\n",
    "        return x\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "@FunctionTransformer\n",
    "@copy_and_return_x\n",
    "def custom_preprocessor(X, y=None) -> None:\n",
    "    X = X.join(cc, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # On commence par réparer l'erreur que nous avons vu à l'étape d'Analyse Exploratoire\n",
    "    X['DAYS_EMPLOYED'] = X['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "    # On va ensuite supprimer la variable identifiant\n",
    "    X.pop('SK_ID_CURR')  # TODO: Check for this\n",
    "\n",
    "    # On va enfin traiter les valeurs catégorielles\n",
    "\n",
    "    # TODO: Vaut-il mieux utiliser trois colonnes?\n",
    "    X['CODE_GENDER'] = X['CODE_GENDER'].map({'F': 1, 'M': -1}).fillna(0)\n",
    "\n",
    "    for flag_name in X.columns[X.columns.str.startswith('FLAG_')]:\n",
    "        values = set(X[flag_name])\n",
    "        if {0, 1} - values:\n",
    "            continue\n",
    "        if {\"Y\", \"N\"} - values:\n",
    "            X[flag_name] = X[flag_name].map({'Y': 1, 'N': 0})\n",
    "        else:\n",
    "            raise RuntimeError(flag_name, values)\n",
    "\n",
    "\n",
    "@FunctionTransformer\n",
    "@copy_and_return_x\n",
    "def sanitize_feature_names(x, y=None):\n",
    "    x.columns = [re.sub(r'[^A-Za-z0-9_]+', '', str(col)) for col in x.columns.tolist()]\n",
    "\n",
    "\n",
    "# 2. Build the pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('custom_preprocessor', custom_preprocessor),\n",
    "    ('occupation_pca', OccupationPCA()),\n",
    "    ('first_name_sanitization', deepcopy(sanitize_feature_names)),\n",
    "    ('dummy_preprocessor', CategoricalEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "    ('second_name_sanitization', deepcopy(sanitize_feature_names)),\n",
    "], verbose=True)\n",
    "\n",
    "# We try to see if the pipeline works as expected\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "deepcopy(preprocessing_pipeline).fit_transform(X_train, y_train)"
   ],
   "id": "a2c156e3d6ffbd46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8fcb8125ba9d4c9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Partie 4: Outils",
   "id": "bc939837b9eb8787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Danger of this, if values are negatives and test values are smaller than train values, offset won't be enough\n",
    "# I don't want the BoxCox transformer to mess up data that it cannot fix.\n",
    "# TODO: Review how I estimate if it's worth the transformation\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "class ConditionalBoxCox(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, p_value_threshold: float = 0.05):\n",
    "        self.p_value_threshold = p_value_threshold\n",
    "        self.columns_to_transform_ = []\n",
    "        self.learned = {}\n",
    "\n",
    "    def _as_numpy(self, X: pd.DataFrame | np.ndarray[tuple[int, ...], np.dtype[np.float64]]):\n",
    "        x: np.ndarray\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            for col in X.columns:\n",
    "                if X[col].isna().any():\n",
    "                    raise NotImplementedError(\n",
    "                        'I do not know yet how this would be handled,\\n\\t'\n",
    "                        'for now, you need this to follow an inputer to have no null values')\n",
    "            columns_to_drop = [i for i in X.columns if X[i].dtype == 'object']\n",
    "            columns_to_keep = [i for i in X.columns if X[i].dtype in ('float64', 'int64')]\n",
    "            if len(other_columns := set(X.columns) - set(columns_to_keep) - set(columns_to_drop)):\n",
    "                raise NotImplementedError(other_columns)\n",
    "            x = X.to_numpy()\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            x = X\n",
    "        else:\n",
    "            raise NotImplementedError(type(X))\n",
    "        return x\n",
    "\n",
    "    # noinspection PyTypeHints\n",
    "    def fit(self, X: pd.DataFrame | np.ndarray[tuple[int, ...], np.dtype[np.float64]], y=None):\n",
    "        x = self._as_numpy(X.copy())\n",
    "\n",
    "        for col, data in enumerate(x.T):\n",
    "            if len(set(data)) < 10:  # Not worth the change\n",
    "                continue\n",
    "            data = data + (shift := epsilon - (0 if (data > 0).all() else data.min()))\n",
    "            _, initial_p = stats.shapiro(sample(list(data), 100))\n",
    "            transformed_data, ld = stats.boxcox(data)\n",
    "            _, transformed_p = stats.shapiro(sample(list(transformed_data), 100))\n",
    "            if transformed_p > initial_p and transformed_p > self.p_value_threshold:\n",
    "                self.columns_to_transform_.append(col)\n",
    "                self.learned[col] = ld, shift\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x = self._as_numpy(X.copy())\n",
    "        for index in self.columns_to_transform_:\n",
    "            ld, shift = self.learned[index]\n",
    "            x[:, index] = stats.boxcox(\n",
    "                x[:, index] + shift, lmbda=ld)\n",
    "        return x\n",
    "\n",
    "\n",
    "def new_pipeline(*steps):\n",
    "    return Pipeline(deepcopy(preprocessing_pipeline.steps) + list(steps))"
   ],
   "id": "379a1e1a0666c4b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 5: Pipelines de modélisation\n",
    "\n",
    "Nous allons maintenant définir nos modèles et lancer les expérimentations. Chaque modèle sera testé dans un `run` MLflow distinct.\n",
    "\n",
    "Pour chaque `run`, nous allons logger :\n",
    "1.  **Les paramètres** : Tous les hyperparamètres du pipeline.\n",
    "2.  **Les métriques** : Les scores (AUC, coût métier, seuil) calculés par validation croisée (moyenne et écart-type).\n",
    "3.  **Le modèle final** : Le pipeline complet, entraîné sur l'ensemble des données, prêt à être déployé.\n",
    "4.  **(Optionnel) Des artefacts** : Comme des graphiques (ex: feature importance)."
   ],
   "id": "525922cf91034380"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from mlflow import set_experiment, start_run, log_params, log_metric, log_metrics, sklearn, models, set_tag\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Configuration et Préparation ---\n",
    "\n",
    "# Définition des modèles de prétraitement\n",
    "pipelines = [\n",
    "    ('DummyClassifier', new_pipeline(('model', DummyClassifier(strategy='stratified'))), {}),\n",
    "    ('LogisticRegression', new_pipeline(\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('coxbox', ConditionalBoxCox()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(class_weight='balanced'))), {\n",
    "         'model__C': [0.1, 1.0, 10.0], 'model__max_iter': [100, 1000]}),\n",
    "    ('LightGBMClassifier', new_pipeline(('model', LGBMClassifier(class_weight='balanced'))), {\n",
    "        'model__n_estimators': [100, 200], 'model__learning_rate': [0.05, 0.1], 'model__num_leaves': [20, 31]})\n",
    "]\n",
    "\n",
    "# Définition de la stratégie de validation croisée\n",
    "cv_strategy = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=random_state)\n",
    "\n",
    "# Préparation des données\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "\n",
    "display(Markdown(f\"***{len(pipelines)}*** types de modèles sont prêts à être testés.\"))\n",
    "\n",
    "# Configuration de MLflow\n",
    "set_tracking_uri('http://127.65.12.247:50222')\n",
    "set_experiment(\"P7_Credit_Scoring_Models\")\n",
    "\n",
    "# Variables pour suivre le meilleur modèle sur l'ensemble des expérimentations\n",
    "best_global_score = -float('inf')  # On maximise le score (qui est un coût négatif)\n",
    "best_global_model_name = \"\"\n",
    "best_global_pipeline_base = None\n",
    "best_global_params = {}\n",
    "\n",
    "# Boucle sur chaque type de modèle (Dummy, LogisticRegression, LightGBM)\n",
    "for model_name, pipeline_base, grid_params in pipelines:\n",
    "    print(f\"\\n--- Lancement de l'expérimentation pour le modèle : {model_name} ---\")\n",
    "\n",
    "    # On crée un \"run parent\" pour regrouper tous les tests d'hyperparamètres de ce modèle\n",
    "    with start_run(run_name=f\"{model_name}_Hyperparameter_Search\") as parent_run:\n",
    "        set_tag(\"model_type\", model_name)\n",
    "\n",
    "        grid = list(ParameterGrid(grid_params)) or [{}]\n",
    "\n",
    "        # Variables pour suivre le meilleur run localement (pour ce type de modèle)\n",
    "        best_local_score = -float('inf')\n",
    "        best_local_params = {}\n",
    "\n",
    "        # Boucle sur chaque combinaison d'hyper paramètres\n",
    "        for parameters_index, parameters_data in enumerate(grid):\n",
    "\n",
    "            # Pour chaque jeu de paramètres, on crée un \"run enfant\" imbriqué\n",
    "            # C'est la correction clé : chaque test a son propre run !\n",
    "            with start_run(run_name=f\"run_{parameters_index + 1}\", nested=True) as child_run:\n",
    "                print(f\"  > Test des paramètres : {parameters_data}\")\n",
    "\n",
    "                # 1. Log des paramètres (une seule fois par run enfant)\n",
    "                log_params(parameters_data)\n",
    "\n",
    "                pipeline = deepcopy(pipeline_base)\n",
    "                pipeline.set_params(**parameters_data)\n",
    "\n",
    "                # 2. Cross-validation\n",
    "                cv_results = cross_validate(\n",
    "                    estimator=pipeline,\n",
    "                    X=X_train,\n",
    "                    y=y_train,\n",
    "                    cv=cv_strategy,\n",
    "                    scoring=scoring,\n",
    "                    n_jobs=-1,\n",
    "                    return_train_score=False\n",
    "                )\n",
    "\n",
    "                # 3. Log des métriques résumées dans le run enfant\n",
    "                summary_metrics = {}\n",
    "                for scorer_name in scoring.keys():\n",
    "                    mean_score = np.mean(cv_results[f'test_{scorer_name}'])\n",
    "                    std_score = np.std(cv_results[f'test_{scorer_name}'])\n",
    "                    summary_metrics[f'mean_{scorer_name}'] = mean_score\n",
    "                    summary_metrics[f'std_{scorer_name}'] = std_score\n",
    "\n",
    "                summary_metrics['mean_fit_time'] = np.mean(cv_results['fit_time'])\n",
    "                log_metrics(summary_metrics)\n",
    "\n",
    "                # 4. Suivi du meilleur score local\n",
    "                current_score = summary_metrics['mean_business_cost']\n",
    "                if current_score > best_local_score:\n",
    "                    best_local_score = current_score\n",
    "                    best_local_params = parameters_data\n",
    "\n",
    "        # 5. Une fois tous les hyper paramètres testés, on met à jour le run parent avec le meilleur résultat\n",
    "        log_metric(\"best_mean_business_cost\", best_local_score, run_id=parent_run.info.run_id)\n",
    "        log_params(best_local_params, run_id=parent_run.info.run_id)\n",
    "\n",
    "        # 6. Mise à jour du meilleur score global\n",
    "        if best_local_score > best_global_score:\n",
    "            print(f\"  > Nouveau meilleur modèle GLOBAL trouvé : {model_name} avec un coût de {-best_local_score:.2f}\")\n",
    "            best_global_score = best_local_score\n",
    "            best_global_model_name = model_name\n",
    "            best_global_pipeline_base = pipeline_base\n",
    "            best_global_params = best_local_params\n",
    "\n",
    "print(\"\\n\\n--- FIN DE LA RECHERCHE D'HYPERPARAMÈTRES ---\")\n",
    "print(f\"Le meilleur modèle global est : '{best_global_model_name}'\")\n",
    "print(f\"Avec les hyperparamètres : {best_global_params}\")\n",
    "# On affiche le coût en positif pour la lisibilité\n",
    "print(f\"Son coût métier moyen par validation croisée est de : {-best_global_score:.2f}\")\n",
    "\n",
    "# --- 3. Entraînement et enregistrement du modèle champion ---\n",
    "# Cette partie est maintenant plus claire : on prend le meilleur pipeline et on l'entraîne une dernière fois\n",
    "print(\"\\n--- Entraînement final et enregistrement du modèle champion ---\")\n",
    "\n",
    "if best_global_pipeline_base:\n",
    "    # On crée un dernier run dédié au modèle final\n",
    "    with start_run(run_name=f\"{best_global_model_name}_champion\") as champion_run:\n",
    "        print(f\"Création d'un run final pour le champion : {champion_run.info.run_name}\")\n",
    "\n",
    "        # On loggue ses meilleurs paramètres et son score\n",
    "        log_params(best_global_params)\n",
    "        log_metric(\"final_cv_business_cost\", -best_global_score)  # On log le coût positif\n",
    "        set_tag(\"model_status\", \"champion\")\n",
    "\n",
    "        # On configure le pipeline final avec les meilleurs paramètres\n",
    "        champion_pipeline = deepcopy(best_global_pipeline_base)\n",
    "        champion_pipeline.set_params(**best_global_params)\n",
    "\n",
    "        # On l'entraîne sur TOUTES les données d'entraînement\n",
    "        champion_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # On loggue le modèle entraîné dans MLflow pour pouvoir le réutiliser plus tard\n",
    "        X_test_sample = test.drop('TARGET', axis=1).head()\n",
    "        signature = models.infer_signature(X_test_sample, champion_pipeline.predict_proba(X_test_sample))\n",
    "\n",
    "        sklearn.log_model(\n",
    "            sk_model=champion_pipeline,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,\n",
    "            registered_model_name=f\"{best_global_model_name}_champion_model\"\n",
    "        )\n",
    "        print(\"Modèle champion entraîné et enregistré dans le Model Registry de MLflow.\")\n",
    "\n",
    "        # On stocke le meilleur pipeline pour les cellules suivantes (par ex. pour SHAP)\n",
    "        best_model = champion_pipeline\n",
    "\n",
    "else:\n",
    "    print(\"Aucun modèle n'a été entraîné avec succès.\")"
   ],
   "id": "803ab883f5964312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('TARGET')\n",
    "best_model.fit(X_train, y_train)\n",
    "X_test = test.copy()\n",
    "y_test = X_test.pop('TARGET')\n",
    "RocCurveDisplay.from_estimator(best_model, X_test, y_test, ax=ax)\n",
    "plt.title(\"Courbe ROC du meilleur modèle\")\n",
    "\n",
    "# Sauvegarde la figure en tant qu'artefact MLflow\n",
    "log_figure(fig, \"roc_curve.png\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "6541e9ea580478dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import mlflow\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, RocCurveDisplay, roc_auc_score, accuracy_score, f1_score\n",
    "from mlflow import end_run\n",
    "\n",
    "# TODO: Why this?\n",
    "end_run()\n",
    "\n",
    "# --- 1. Récupération du modèle champion et du run MLflow associé ---\n",
    "# On suppose que la cellule précédente a déjà entraîné et stocké le meilleur modèle\n",
    "# dans la variable 'best_model'.\n",
    "# On récupère les informations du dernier run \"CHAMPION\" qui a été créé.\n",
    "champion_run = mlflow.last_active_run()\n",
    "run_id = champion_run.info.run_id\n",
    "print(f\"Analyse du modèle champion depuis le run MLflow ID: {run_id}\")\n",
    "\n",
    "# --- 2. Préparation des données de test ---\n",
    "# Utilisons les variables train_df/test_df que vous avez créées au début avec train_test_split.\n",
    "# (Rappel : il faudra adapter ceci après l'ajout du feature engineering)\n",
    "X_test_final = test_df.drop('TARGET', axis=1)\n",
    "y_test_final = test_df['TARGET']\n",
    "\n",
    "# --- 3. Évaluation sur le jeu de test et logging des métriques ---\n",
    "print(\"Évaluation sur le jeu de test...\")\n",
    "y_pred_proba_test = best_model.predict_proba(X_test_final)\n",
    "\n",
    "# On utilise la fonction 'find_optimal_threshold' que vous avez définie plus haut\n",
    "optimal_threshold_test, business_cost_test = find_optimal_threshold(y_test_final, y_pred_proba_test)\n",
    "y_pred_class_test = (y_pred_proba_test[:, 1] >= optimal_threshold_test).astype(int)\n",
    "\n",
    "test_metrics = {\n",
    "    \"test_business_cost\": business_cost_test,\n",
    "    \"test_optimal_threshold\": optimal_threshold_test,\n",
    "    \"test_roc_auc\": roc_auc_score(y_test_final, y_pred_proba_test[:, 1]),\n",
    "    \"test_accuracy_at_threshold\": accuracy_score(y_test_final, y_pred_class_test),\n",
    "    \"test_f1_at_threshold\": f1_score(y_test_final, y_pred_class_test)\n",
    "}\n",
    "\n",
    "# On loggue les métriques DANS LE RUN EXISTANT\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.log_metrics(test_metrics)\n",
    "    print(\"Métriques de test ajoutées au run champion.\")\n",
    "\n",
    "    # --- 4. Logging des artefacts (graphiques) ---\n",
    "    # Matrice de confusion\n",
    "    fig_cm, ax_cm = plt.subplots()\n",
    "    cm = confusion_matrix(y_test_final, y_pred_class_test)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax_cm, cmap='Blues')\n",
    "    ax_cm.set_title(f'Matrice de Confusion (Seuil = {optimal_threshold_test:.2f})')\n",
    "    mlflow.log_figure(fig_cm, \"test_confusion_matrix.png\")\n",
    "    plt.close(fig_cm)\n",
    "\n",
    "    # Courbe ROC\n",
    "    fig_roc, ax_roc = plt.subplots()\n",
    "    RocCurveDisplay.from_estimator(best_model, X_test_final, y_test_final, ax=ax_roc)\n",
    "    ax_roc.set_title(\"Courbe ROC sur jeu de test\")\n",
    "    mlflow.log_figure(fig_roc, \"test_roc_curve.png\")\n",
    "    plt.close(fig_roc)\n",
    "    print(\"Graphiques (matrice de confusion, courbe ROC) ajoutés au run champion.\")\n",
    "\n",
    "    # --- 5. Analyse SHAP ---\n",
    "    print(\"Calcul des valeurs SHAP...\")\n",
    "    preprocessor = best_model[:-1]\n",
    "    model_step = best_model.named_steps['model']\n",
    "\n",
    "    # On utilise un échantillon pour la rapidité\n",
    "    X_test_sample = X_test_final # .sample(n=1000, random_state=random_state)\n",
    "    X_test_transformed = preprocessor.transform(X_test_sample)\n",
    "    X_test_transformed.columns = [str(col) for col in X_test_transformed.columns]\n",
    "\n",
    "    explainer = None\n",
    "    if 'LGBMClassifier' in str(type(model_step)):\n",
    "        explainer = shap.TreeExplainer(model_step)\n",
    "    elif 'LogisticRegression' in str(type(model_step)):\n",
    "        explainer = shap.LinearExplainer(model_step, X_test_transformed)\n",
    "\n",
    "    if explainer:\n",
    "        shap_values = explainer(X_test_transformed)\n",
    "        shap.summary_plot(shap_values, X_test_transformed, show=False, max_display=20, plot_type=\"bar\")\n",
    "        plt.title(\"Importance globale des features (SHAP) sur le jeu de test\")\n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(plt.gcf(), \"test_shap_summary_bar.png\")\n",
    "        plt.close()\n",
    "        print(\"Graphique SHAP ajouté au run champion.\")\n",
    "\n",
    "print(\"\\n--- ANALYSE TERMINÉE ---\")\n",
    "print(f\"Le run {run_id} a été mis à jour avec les analyses sur le jeu de test.\")"
   ],
   "id": "da4fcba41518f7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 'explainer' et 'X_test_transformed' sont déjà calculés dans ton code\n",
    "# Choisissons un client à expliquer (par exemple le premier)\n",
    "client_index = 0\n",
    "shap_values_client = explainer(X_test_transformed.iloc[[client_index]])\n",
    "\n",
    "# Pour un graphique en \"cascade\" (très visuel)\n",
    "shap.waterfall_plot(shap_values_client[0])"
   ],
   "id": "5ea500a559a09a7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partie 4 - Analyse des Résultats et Prochaines Étapes\n",
    "\n",
    "Toutes nos expérimentations sont maintenant enregistrées. Pour les visualiser, lance l'interface utilisateur de MLflow via \"http://127.65.12.247:50222\"\n",
    "\n",
    "\n",
    "Dans l'interface, tu pourras :\n",
    "- **Comparer les modèles** en les sélectionnant et en cliquant sur \"Compare\".\n",
    "- **Trier les runs** par `business_cost_mean` (le score le plus bas est le meilleur) ou `roc_auc_mean`.\n",
    "- **Inspecter chaque run** pour voir les paramètres exacts qui ont produit un certain résultat.\n",
    "\n",
    "### Prochaines étapes basées sur ces résultats :\n",
    "\n",
    "1.  **Choisir le meilleur modèle** : Sur la base du score métier, sélectionne le modèle le plus prometteur (probablement LightGBM).\n",
    "2.  **Optimisation des hyperparamètres** : Pour le modèle choisi, tu pourras lancer une nouvelle série d'expérimentations (par exemple avec `GridSearchCV` ou `Hyperopt`) pour trouver les meilleurs hyperparamètres, toujours en optimisant le score métier. Chaque essai de la recherche d'hyperparamètres peut être un `run` MLflow imbriqué !\n",
    "3.  **Analyse de l'importance des features** : Une fois le modèle finalisé, utilise des librairies comme **SHAP** pour analyser l'importance des features globale et locale, comme demandé dans le brief.\n",
    "4.  **Enregistrement du modèle final** : Le meilleur run pourra être promu dans le \"Model Registry\" de MLflow, lui donnant un statut (Staging, Production) pour le déploiement."
   ],
   "id": "c14f928412b25f37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
