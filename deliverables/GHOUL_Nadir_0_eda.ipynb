{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from contextlib import suppress\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from pathlib import Path\n",
    "\n",
    "from requests import get, Response\n",
    "from hashlib import sha256\n",
    "from tqdm.notebook import tqdm\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from utils.image_inverter import save as save\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# For pretty writing\n",
    "from IPython.display import display, Markdown\n"
   ],
   "id": "6d07a380490ec8e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Analyse exploratoire de données (Exploratory Data Analysis)\n",
   "id": "53e0ebae05af9b0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Partie 0 - Outils",
   "id": "488b77bf5cbee800"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "graph_folder: Path = Path(\"./graphs\")\n",
    "\n",
    "\n",
    "def save_figure(figure: plt.Figure, folder: str, figure_name: str) -> None:\n",
    "    folder = graph_folder / folder\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    save(figure, folder / f'{figure_name}.png')\n",
    "\n",
    "\n",
    "_cache_folder = Path('~/.cache/gn_p7').expanduser()\n",
    "_cache_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_ds_url = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip'\n",
    "\n",
    "\n",
    "def download(url: str) -> Path:\n",
    "    url_id: str = sha256(url.encode('utf-8')).hexdigest()\n",
    "    local_path: Path = _cache_folder / url_id\n",
    "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not local_path.exists():\n",
    "        tmp_path: Path = _cache_folder / (url_id + '.tmp')\n",
    "        res: Response = get(url, stream=True)\n",
    "        with tmp_path.open('wb') as f, tqdm(\n",
    "                total=int(res.headers.get('content-length')),\n",
    "                desc=f'Downloading {url}',\n",
    "                unit_scale=True) as q:\n",
    "            for chunk in res.iter_content(chunk_size=8192):\n",
    "                q.update(len(chunk))\n",
    "                f.write(chunk)\n",
    "        tmp_path.replace(local_path)\n",
    "    return local_path\n",
    "\n",
    "\n",
    "def download_zip_archive(url: str) -> Path:\n",
    "    \"\"\"Download a zip archive, extract it then return the folder containing its content\"\"\"\n",
    "    archive_path: Path = download(url)\n",
    "    archive_folder: Path = Path(archive_path.as_posix() + '.dir')\n",
    "\n",
    "    if not archive_folder.exists():\n",
    "        print(f'Extracting archive {url}...', flush=True)\n",
    "        archive_temp: Path = Path(archive_path.as_posix() + '.tmp')\n",
    "        archive_temp.mkdir(parents=True, exist_ok=True)\n",
    "        archive: ZipFile = ZipFile(archive_path)\n",
    "        archive.extractall(path=archive_temp)\n",
    "        archive_temp.replace(archive_folder)\n",
    "        print(f'Extracting archive {url}...done', flush=True)\n",
    "\n",
    "    return archive_folder\n",
    "\n",
    "\n",
    "datasets: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "\n",
    "def get_dataset(name: str) -> pd.DataFrame:\n",
    "    folder = download_zip_archive(_ds_url)\n",
    "    if not name.endswith('.csv'):\n",
    "        name = f'{name}.csv'\n",
    "    try:\n",
    "        return datasets[name]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            _df = pd.read_csv(folder / name)\n",
    "        except FileNotFoundError:\n",
    "            display(Markdown(f'# ERROR: Dataset {name!r} not found, available datasets are:\\n' + '\\n'.join(\n",
    "                f'- {p.name}' for p in sorted(folder.iterdir(), key=(lambda x: x.name.lower())))))\n",
    "            raise KeyError(name) from None\n",
    "        else:\n",
    "            datasets[name] = _df\n",
    "            return _df.copy()\n"
   ],
   "id": "28d5c64270036d47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T20:00:10.657551Z",
     "start_time": "2025-08-17T20:00:10.654977Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Partie 1 - Applications\n",
    "Notre dataset contiens plusieurs fichiers.\n",
    "Nous feront comme le notebook exemple, en commençant initialement avec application_{train,test}, puis en rajoutant des données pour améliorer la performance du modèle."
   ],
   "id": "ec3ddeca5506f92f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T20:05:23.859171Z",
     "start_time": "2025-08-17T20:05:22.507197Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Partie 1.1 - Données brutes\n",
    "\n",
    "Il serai peut-être nécéssaire de prétraiter les données, cependant je vais tenter de créer un modèle initial sur les données brutes pour servir de référence"
   ],
   "id": "cef99880944cab06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1.1 - k-Means\n",
    "\n",
    "k-Means est linéaire, et non supervisé, il aura probablement un score assez faible"
   ],
   "id": "8849355abea49ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scoring related\n",
    "\n",
    "\n",
    "\n",
    "def heat_map(matrix: np.ndarray, classes: list[str], title: str, y_label: str, x_label: str, fmt: str) -> None:\n",
    "    sns.heatmap(matrix, annot=True, fmt=fmt, xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "def confusion(\n",
    "        y_truth: pd.Series,\n",
    "        y_prediction: pd.Series,\n",
    "        model_name: str,\n",
    ") -> dict:\n",
    "    assert len(y_truth) == len(y_prediction), \"Input Series must have the same length.\"\n",
    "    classes = sorted(set(y_truth) | set(y_prediction))\n",
    "    matrix = confusion_matrix(y_truth, y_prediction, labels=classes)\n",
    "    sns.heatmap(matrix / len(y_truth), annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Confusion Matrix for {model_name!r}', fontsize=16)\n",
    "    plt.ylabel('Actual Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    return {\n",
    "        # TODO: More metrics\n",
    "        'accuracy': accuracy_score(y_truth, y_prediction),\n",
    "        'precision': precision_score(y_truth, y_prediction, zero_division=0),\n",
    "        'recall': recall_score(y_truth, y_prediction, zero_division=0),\n",
    "        'f1_score': f1_score(y_truth, y_prediction, zero_division=0)\n",
    "    }\n",
    "def confusion(\n",
    "        y_truth: pd.Series,\n",
    "        y_prediction: pd.Series,\n",
    "        model_name: str,\n",
    ") -> dict:\n",
    "    assert len(y_truth) == len(y_prediction), \"Input Series must have the same length.\"\n",
    "    classes = sorted(set(y_truth) | set(y_prediction))\n",
    "    heat_map(\n",
    "        (matrix := confusion_matrix(y_truth, y_prediction, labels=classes)),\n",
    "        classes, f'Confusion Matrix for {model_name!r} (absolute values)', 'Actual Label', 'Predicted Label', 'd')\n",
    "\n",
    "    # 'matrix' est la matrice de confusion non normalisée\n",
    "    # On divise chaque valeur par la somme de sa ligne\n",
    "    # np.newaxis est crucial pour que la division se fasse correctement\n",
    "    heat_map(\n",
    "        (100 * (matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis])),\n",
    "        classes, f'Confusion Matrix for {model_name!r} (percentages)', 'Actual Label', 'Predicted Label', '.02f')\n",
    "\n",
    "    return {\n",
    "        # TODO: More metrics\n",
    "        'accuracy': accuracy_score(y_truth, y_prediction),\n",
    "        'precision': precision_score(y_truth, y_prediction, zero_division=0),\n",
    "        'recall': recall_score(y_truth, y_prediction, zero_division=0),\n",
    "        'f1_score': f1_score(y_truth, y_prediction, zero_division=0)\n",
    "    }\n",
    "\n",
    "\n",
    "def k_means():\n",
    "    display(Markdown('# WARNING: k-Means distinguish between two classes, but might switch classes'))\n",
    "    X = get_dataset('application_train.csv')\n",
    "    y_true = X.pop('TARGET')\n",
    "    for column in list(X.columns):\n",
    "        try:\n",
    "            X[column] = X[column].astype(float).fillna(0)\n",
    "        except ValueError:\n",
    "            X.pop(column)\n",
    "    model = KMeans(len(y_true.unique()))\n",
    "    y_pred = model.fit(X).predict(X)\n",
    "    return confusion(y_true, y_pred, 'k-Means')\n",
    "\n",
    "\n",
    "k_means()"
   ],
   "id": "cd2b9a49cdb2d292",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Similar to plt.hist, but automatically labels the axes\n",
    "plt.show(sns.countplot(data=(tmp := get_dataset('application_train.csv')), x='TARGET').set_title(\n",
    "    'Distribution of Target Values in the training dataset').figure)\n",
    "\n",
    "display(Markdown(\n",
    "    '<b>Nous pouvons voir ici un clair déséquilibre des classes, en effet, '\n",
    "    f'seuls {100 * sum(tmp.TARGET) / len(tmp.TARGET):.2f}% des clients ont difficultés de paiement</b><br>'\n",
    "))"
   ],
   "id": "364e6cfbba3f9ce2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1866b03f58d0a24a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1.2 Start Here: A Gentle Introduction\n",
    "\n",
    "Je vais ici tester les méthodes proposées par le notebook de référence.\n",
    "Le but étant de m'acclimater aux outils proposés."
   ],
   "id": "a1dfb926a1080cd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T10:52:44.649665Z",
     "start_time": "2025-08-18T10:52:44.647651Z"
    }
   },
   "cell_type": "markdown",
   "source": "#### Importation des librairies utilisées plus bas",
   "id": "ef45d4a674d570bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "3b2d7f619b79cb4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Chargement des deux datasets",
   "id": "57e46b7163496714"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(app_train := get_dataset('application_train'))",
   "id": "7891b2744c2b1d0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(app_test := get_dataset('application_test'))",
   "id": "2c0b8997cb44ea1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Traitement des valeurs manquantes (déséquilibre TARGET déjà vu plus tôt)",
   "id": "850c73b07963c7a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to calculate missing values by column# Funct\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns={0: 'Missing Values', 1: '% of Total Values'})\n",
    "\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "                                                              \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "# Missing values statistics\n",
    "(missing_values := missing_values_table(app_train))"
   ],
   "id": "d4ec5718ebadb0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for fold in ('train', 'test'):\n",
    "    missing_test_values = missing_values_table(get_dataset('application_' + fold + '.csv'))\n",
    "\n",
    "    # TODO: Set the plot style for dark mode when exporting to png\n",
    "    plt.figure(figsize=(16, 12))  # There are a lot of columns\n",
    "    sns.barplot(x=missing_test_values['% of Total Values'], y=missing_test_values.index)\n",
    "    plt.title(f'Percentage of Missing Values by Feature ({fold.title()}ing fold)', fontsize=16)\n",
    "    plt.xlabel('% of Total Values', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "    # Add percentage text on the bars\n",
    "    for index, value in enumerate(missing_test_values['% of Total Values']):\n",
    "        plt.text(value, index, f' {value}%', va='center')\n",
    "\n",
    "    plt.xlim(0, 110)  # Set x-limit to give space for text\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test.png')\n",
    "    plt.show()"
   ],
   "id": "6c6e4da928693c49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import missingno as msno\n",
    "\n",
    "for fold in ('train', 'test'):\n",
    "    msno.matrix(get_dataset('application_' + fold + '.csv'), fontsize=12)\n",
    "    plt.title(f'Missing Values Count ({fold.title()}ing fold)', fontsize=16)\n",
    "    plt.show()"
   ],
   "id": "9bb8f2dc1822f72e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import missingno as msno\n",
    "\n",
    "for fold in ('train', 'test'):\n",
    "    df = get_dataset('application_' + fold + '.csv')\n",
    "    msno.matrix(df[list(sorted(df.columns, key=(lambda col: int(df[col].notna().sum()))))], fontsize=12)\n",
    "    plt.title(f'Missing Values Count ({fold.title()}ing fold)', fontsize=16)\n",
    "    plt.show()"
   ],
   "id": "46b29e8385b7a6d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On peut voir ce qui a été montré précédemment, la moitié des colonnes ne possèdent aucunes valeurs manquantes, et le reste va grossièrement de 40 à 60 pourcent de valeurs manquantes",
   "id": "a11e0d4a3c00e41b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: The notebook suggests XGBoost to get away with the use of a dataset with missing values, I should probably look for all the models and split them into two groups, one that I will use first after filling the voids, and the second one that will get the unprocessed dataset, to see if the imputation adds or reduces accuracy",
   "id": "6bd665722bfaf2d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_test_diff_col, = set(app_train) - set(app_test)\n",
    "display(Markdown(f'La colonne **{train_test_diff_col!r}** est la seule à manquer du dataset de test'))"
   ],
   "id": "fa29a278fce14acc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert not app_train.TARGET.isna().sum()\n",
    "display(Markdown('Il ne manque aucune target'))"
   ],
   "id": "eebfe133f4bef2ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Types de données\n",
    "\n",
    "Les colonnes étant communes entre train et test, je vais me contenter de voir les types des valeurs de training\n",
    "\n",
    "TODO: Combiner les deux durant l'analyse, ou garder une séparation stricte?"
   ],
   "id": "d9858ad8fdf0d12b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Number of each type of column\n",
    "app_train.dtypes.value_counts()"
   ],
   "id": "2bd76fe5375f560d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "app_train[app_train.columns[app_train.dtypes == 'float64']]",
   "id": "f9234a49ee130f03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "app_train[app_train.columns[app_train.dtypes == 'int64']]",
   "id": "34b1d124100c4de4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Le type objet peut être associée avec une colonne numérique possédant quelques valeurs invalides, il est donc important de m'assurer que ce n'est pas le cas ici",
   "id": "b7b38d8d9990bf67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "app_train[app_train.columns[app_train.dtypes == 'object']]",
   "id": "4829607be7b89e7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "app_train[app_train.columns[app_train.dtypes == 'object']].value_counts()",
   "id": "2c626ffb88a5fab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.DataFrame(\n",
    "    data=[[col_name, len(app_train[col_name].unique())] for col_name in\n",
    "          app_train.columns[app_train.dtypes == 'object']],\n",
    "    columns=['column', 'unique_count'])"
   ],
   "id": "efb0621f72baba48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tmp = pd.DataFrame(\n",
    "    data=[[col_name, len(app_train[col_name].unique())] for col_name in\n",
    "          app_train.columns[app_train.dtypes == 'object']],\n",
    "    columns=['column', 'unique_count'])\n",
    "# TODO: Set the plot style for dark mode when exporting to png\n",
    "plt.figure(figsize=(16, 12))  # There are a lot of columns\n",
    "sns.barplot(x='unique_count', y='column', data=tmp)\n",
    "plt.title(f'Percentage of Missing Values by Feature ({fold.title()}ing fold)', fontsize=16)\n",
    "plt.xlabel('% of Total Values', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n"
   ],
   "id": "fc3795086547de19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On peut voir relativement peu de types de valeurs textuelle, ce qui est bon si nous décitons d'utiliser des méthodes comme OneHot<br>\n",
    "\n",
    "TODO: Voir s'il est possible de transformer les flags et booleans en int<br>\n",
    "TODO: Voir s'il est possible de transformer les code gender -1 et 1 pour homme/femme et 0 pour XNA<br>\n",
    "TODO: Voir d'autres algorithmes à utiliser pour transformer texte en nombre<br>\n",
    "TODO: Voir la liste des algo supportant les données textuelles (je crois que RandomForest supporte)<br>\n",
    "TODO: Should we do a PCA over the OneHot encoded organization type / occupation type?"
   ],
   "id": "1e6d6ae91c886c33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert not len(app_train.columns[\n",
    "                   (app_train.dtypes != 'int64') &\n",
    "                   (app_train.dtypes != 'float64') &\n",
    "                   (app_train.dtypes != 'object')]), 'Plus de types de colonnes sont présentes'"
   ],
   "id": "5d2c8a1efc10861e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_train = app_train.copy()\n",
    "num_test = app_test.copy()\n",
    "del app_train, app_test\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in num_train:\n",
    "    if num_train[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(num_train[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(num_train[col])\n",
    "            # Transform both training and testing data\n",
    "            num_train[col] = le.transform(num_train[col])\n",
    "            num_test[col] = le.transform(num_test[col])\n",
    "\n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "\n",
    "# one-hot encoding of categorical variables\n",
    "num_train = pd.get_dummies(num_train)\n",
    "num_test = pd.get_dummies(num_test)\n",
    "\n",
    "print('Training Features shape: ', num_train.shape)\n",
    "print('Testing Features shape: ', num_test.shape)\n",
    "\n",
    "print('%d columns were label encoded.' % le_count)\n",
    "\n",
    "train_labels = num_train['TARGET']\n",
    "\n",
    "# Align the training and testing data, keep only columns present in both dataframes\n",
    "num_train, num_test = num_train.align(num_test, join='inner', axis=1)\n",
    "\n",
    "# Add the target back in\n",
    "num_train['TARGET'] = train_labels\n",
    "\n",
    "print('Training Features shape: ', num_train.shape)\n",
    "print('Testing Features shape: ', num_test.shape)"
   ],
   "id": "52437249e7ba504a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "((num_train['DAYS_BIRTH'] / -365).describe()).to_frame().join(\n",
    "    (num_test['DAYS_BIRTH'] / -365).describe(),\n",
    "    lsuffix='_train',\n",
    "    rsuffix='_test'\n",
    ")"
   ],
   "id": "7a1ff301271b0349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: Est-ce normal d'avoir la même chose?",
   "id": "b42b670c04aa1a57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "((num_train['DAYS_BIRTH'] / -365).describe()).to_frame().reset_index()",
   "id": "4d520091a9cbd124",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "((num_test['DAYS_BIRTH'] / -365).describe()).to_frame().reset_index()",
   "id": "bac0951e7a8b2f86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "((num_train['DAYS_EMPLOYED']).describe()).to_frame().join(\n",
    "    (num_test['DAYS_EMPLOYED']).describe(),\n",
    "    lsuffix='_train',\n",
    "    rsuffix='_test'\n",
    ")"
   ],
   "id": "8733d1f67c9a39ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram (Train)')\n",
    "plt.xlabel('Days Employment')\n",
    "plt.show()\n",
    "num_test['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram (Test)')\n",
    "plt.xlabel('Days Employment')\n",
    "plt.show()"
   ],
   "id": "8ad39aa552a7fa1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7ed3d7b41a7d5a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "anom = num_train[num_train['DAYS_EMPLOYED'] == 365243]\n",
    "non_anom = num_train[num_train['DAYS_EMPLOYED'] != 365243]\n",
    "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\n",
    "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\n",
    "print('There are %d anomalous days of employment' % len(anom))"
   ],
   "id": "6510aa363b525c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Set a new name\n",
    "app_train, app_test = num_train.copy(), num_test.copy()\n",
    "del num_train, num_test\n",
    "\n",
    "# Create an anomalous flag column\n",
    "app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "# Replace the anomalous values with nan\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\n",
    "plt.xlabel('Days Employment')\n",
    "\n",
    "app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "print(\n",
    "    'There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))"
   ],
   "id": "117be493bfbecfc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find correlations with the target and sort\n",
    "correlations = app_train.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n')\n",
    "correlations.tail(15)"
   ],
   "id": "2bd3e96c22f8272f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('\\nMost Negative Correlations:\\n')\n",
    "correlations.head(15)"
   ],
   "id": "28254ffca7fe4272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find the correlation of the positive days since birth and target\n",
    "assert (app_train['DAYS_BIRTH'] > 0).sum() == 0, (\n",
    "    (app_train['DAYS_BIRTH'] < 0).sum())\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"
   ],
   "id": "69718e61d31111b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is the most correlated to the target.\n",
    "The days birth being negative, a positive corellation means that young people are less likely to repay on time"
   ],
   "id": "fdb4f6414c8d2ab2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist(app_train['DAYS_BIRTH'] / -365, edgecolor='k', bins=25)\n",
    "plt.title('Age of Client');\n",
    "plt.xlabel('Age (years)');\n",
    "plt.ylabel('Count');"
   ],
   "id": "6645b974d7165084",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / -365, label='target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / -365, label='target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Age (years)');\n",
    "plt.ylabel('Density');\n",
    "plt.title('Distribution of Ages');"
   ],
   "id": "5f058324dd353384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / -365\n",
    "\n",
    "# Bin the age data\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=np.linspace(20, 70, num=22))\n",
    "display(age_data.head(5))\n",
    "\n",
    "# Group by the bin and calculate averages\n",
    "age_groups = age_data.groupby('YEARS_BINNED').mean()\n",
    "display(age_groups)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation=75);\n",
    "plt.xlabel('Age Group (years)');\n",
    "plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group')"
   ],
   "id": "1d64dc851850cfbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the EXT_SOURCE variables and show correlations, as those are the three least correlated\n",
    "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr()\n",
    "ext_data_corrs"
   ],
   "id": "12745c766f6a1172",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)\n",
    "plt.title('Correlation Heatmap');"
   ],
   "id": "25bf56444a76a75f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The biggest correlation is between the first external source and the age",
   "id": "eb800bb33257d275"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "\n",
    "# iterate through the sources\n",
    "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label='target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label='target == 1')\n",
    "\n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % source)\n",
    "    plt.xlabel('%s' % source);\n",
    "    plt.ylabel('Density');\n",
    "\n",
    "plt.tight_layout(h_pad=2.5)\n"
   ],
   "id": "ebc5732ff3aa0474",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "All are weak according to the most common interpretation of the pearson coefficient\n",
    "\n",
    "- .00-.19 “very weak”\n",
    "- .20-.39 “weak”\n",
    "- .40-.59 “moderate”\n",
    "- .60-.79 “strong”\n",
    "- .80-1.0 “very strong”"
   ],
   "id": "74f99ea0c2b89632"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Copy the data for plotting\n",
    "plot_data = ext_data.drop(columns=['DAYS_BIRTH']).copy()\n",
    "\n",
    "# Add in the age of the client in years\n",
    "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
    "\n",
    "# Drop na values and limit to first 100000 rows\n",
    "plot_data = plot_data.dropna().loc[:100000, :]\n",
    "\n",
    "\n",
    "# Function to calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size=20)\n",
    "\n",
    "\n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data=plot_data, diag_sharey=False,\n",
    "                    hue='TARGET',\n",
    "                    vars=[x for x in list(plot_data.columns) if x != 'TARGET'])\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha=0.2)\n",
    "\n",
    "# This forces the old behavior, scaling each curve independently\n",
    "grid.map_diag(sns.kdeplot, common_norm=False)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r)\n",
    "\n",
    "plt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05)"
   ],
   "id": "407767e7cb84616b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "app_train.head().to_csv()",
   "id": "6337efec90367a5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Leakage\n",
    "\n",
    "On a vu plus tôt une très faible corrélation entre les variables et la TARGET.<br>\n",
    "Un risque possible serai d'avoir une variable d'entrée définies après l'emprunt<br>\n",
    "Cependant les variables EXT_SOURCE_x sont opaques\n",
    "Et les colonnes *_SOCIAL_CIRCLE peuvent ne pas avoir été prises assez tôt"
   ],
   "id": "55040b72681e456d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "app_train",
   "id": "273dfa1c1d2decb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c76cfa9965669247",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
